import json
import os
import re
import subprocess
import sys
import torch
import gc

from utils import _get_ngrams, _get_word_ngrams, cal_rouge
from utils import _read_text_file, _get_url_hashes, check_num_stories

_dm_single_close_quote = u'\u2019'  # unicode
_dm_double_close_quote = u'\u201d'
_END_TOKENS = ['.', '!', '?', '...', "'", "`", '"', _dm_single_close_quote,
               _dm_double_close_quote, ")"]  # acceptable ways to end a sentence

_REMAP = {"-lbr-": "(", "-rbr-": ")", "-lcb-": "{", "-rcb-": "}",
          "-lsb-": "[", "-rsb-": "]", "``": '"', "''": '"', "•": ".", "•": "."}


# These are the number of .story files we expect there to be in cnn_stories_dir and dm_stories_dir
num_expected_cnn_stories = 92579
num_expected_dm_stories = 219506


def tokenize_stories(stories_dir, tokenized_stories_dir):
  """
  Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer.
  The output file are named as the input file, with the .json extention added.
  They are created in data/cnn-dailymail/<data_set_name>_tokenized.

  :param stories_dir:             String. Path to the input directory containing the story files.
  :param tokenized_stories_dir:   String. Path to the output directory for the file containing
                                  the tokeninzed article and abstract.
  """
  print("Preparing to tokenize %s to %s..." %
        (stories_dir, tokenized_stories_dir))
  stories = os.listdir(stories_dir)
  # make IO list file
  print("Making list of files to tokenize...")
  with open("mapping.txt", "w") as f:
    for s in stories:
      f.write("%s\n" % (os.path.join(stories_dir, s)))
  command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit',
             '-ssplit.newlineIsSentenceBreak', 'always', '-filelist', 'mapping.txt', '-outputFormat', 'json', '-outputDirectory', tokenized_stories_dir]
  print("Tokenizing %i files in %s and saving in %s..." %
        (len(stories), stories_dir, tokenized_stories_dir))
  subprocess.call(command)
  print("Stanford CoreNLP Tokenizer has finished.")
  os.remove("mapping.txt")

  # Check that the tokenized stories directory contains the same number of files as the original directory
  num_orig = len(os.listdir(stories_dir))
  num_tokenized = len(os.listdir(tokenized_stories_dir))
  if num_orig != num_tokenized:
    raise Exception("The tokenized stories directory %s contains %i files, but it should contain the same number as %s (which has %i files). Was there an error during tokenization?" % (
        tokenized_stories_dir, num_tokenized, stories_dir, num_orig))
  print("Successfully finished tokenizing %s to %s.\n" %
        (stories_dir, tokenized_stories_dir))


def _fix_missing_period(line):
  """
  Adds a period to a line that is missing a period.
  """
  if "@highlight" in line:
    return line
  if line == "":
    return line
  if line[-1] in _END_TOKENS:
    return line
  # print line[-1]
  return line + " ."


def _clean(x):
    return _fix_missing_period(re.sub(
        r"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''",
        lambda m: _REMAP.get(m.group()), x))


def get_art_abs_from_json(json_file):
  """
  Reads the json file generated by Stanford Core NLP tokenizer.
  Makes two 2D arrays representing the article and the abstract.
  Rows represent sentences. Columns represent a word/token.

  :param json_file:   String. Path to the file resulting of the tokenization.
                      Json format.

  :returns:   Two 2D arrays of strings.
  """
  article_tokens_list = []
  abstract_tokens_list = []
  flag = False
  for sent in json.load(open(json_file))['sentences']:
      tokens = [t['word'] for t in sent['tokens']]
      tokens = [t.lower() for t in tokens]
      if (tokens[0] == '@highlight'):
          flag = True
          continue
      if (flag):
          abstract_tokens_list.append(tokens)
          flag = False
      else:
          article_tokens_list.append(tokens)
  article_tokens_list = [_clean(' '.join(sent)).split()
                         for sent in article_tokens_list]
  abstract_tokens_list = [_clean(' '.join(sent)).split()
                          for sent in abstract_tokens_list]

  return article_tokens_list, abstract_tokens_list


def _greedy_selection(article_tokens_list, abstract_tokens_list, summary_size):
    """
    From an abstract summary, generate an extractive summary for the given article.
    Computes the rouge score with 1 gram and 2 grams between sentences from the
    article and the abstract. Sentence's id with the highest score is added to
    the extractive summary.

    :param article_tokens_list:   List of sentences representing a document.
                                  Each sentence is a list of word.
                                  Sentence are here tokenized by Stanford Core NLP.

    :param abstract_tokens_list:  List of sentence representing the abstract given for the document.
                                  Format is the same as article_tokens_list.

    :returns:         An array of index pointing to the selected sentences in article_tokens_list.
    """
    def _rouge_clean(s):
        return re.sub(r'[^a-zA-Z0-9 ]', '', s)

    max_rouge = 0.0
    abstract = sum(abstract_tokens_list, [])
    abstract = _rouge_clean(' '.join(abstract)).split()
    sents = [_rouge_clean(' '.join(s)).split() for s in article_tokens_list]
    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]
    reference_1grams = _get_word_ngrams(1, [abstract])
    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]
    reference_2grams = _get_word_ngrams(2, [abstract])

    selected = []
    for s in range(summary_size):
        cur_max_rouge = max_rouge
        cur_id = -1
        for i in range(len(sents)):
            if (i in selected):
                continue
            c = selected + [i]
            candidates_1 = [evaluated_1grams[idx] for idx in c]
            candidates_1 = set.union(*map(set, candidates_1))
            candidates_2 = [evaluated_2grams[idx] for idx in c]
            candidates_2 = set.union(*map(set, candidates_2))
            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']
            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']
            rouge_score = rouge_1 + rouge_2
            if rouge_score > cur_max_rouge:
                cur_max_rouge = rouge_score
                cur_id = i
        if (cur_id == -1):
            return selected
        selected.append(cur_id)
        max_rouge = cur_max_rouge

    return sorted(selected)


def gen_oracle_summary(story_file):
  """
    Re-write the abstract summaries to extractive summaries using a greedy algorithm.

    :param story_file:    String. Path to the file with its extension.
  """
  article_tokens_list, abstract_tokens_list = get_art_abs_from_json(story_file)
  oracle_ids = _greedy_selection(article_tokens_list, abstract_tokens_list, 3)
  return article_tokens_list, abstract_tokens_list, oracle_ids


def _get_story_path(token_dirs, story_name):
  """
  Finds the directory in with the story (file name) is stored.
  Returns the full path.

  :param token_dirs:   Array of strings. Path to the directories of tokenized stories.
  :param story_name:  String. Name of the file.

  :returns:   String. Full pat to the story.
  """
  story_file = None
  for td in token_dirs:
    # Look in the tokenized story dirs to find the .story.json file corresponding to this url
    if os.path.isfile(os.path.join(td, story_name)):
      story_file = os.path.join(td, story_name)

  if story_file is None:
    print("Error: Couldn't find tokenized story file %s in either tokenized story directories %s. Was there an error during tokenization?" % (
        story_name, token_dirs))
    # Check again if tokenized stories directories contain correct number of files
    print("Checking that the tokenized stories directories %s contain correct number of files..." % (token_dirs))
    check_num_stories(token_dirs[0], num_expected_cnn_stories)
    check_num_stories(token_dirs[1], num_expected_dm_stories)
    raise Exception("Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither." % (
        token_dirs[0], token_dirs[1], story_name))

  return story_file


def write_chunk(out_dir, chunk, chunk_nb, set_name='chunk'):
  chunk_fname = os.path.join(out_dir, '%s_%03d.json' %
                             (set_name, chunk_nb))  # new chunk
  print('Saving to %s' % chunk_fname)
  torch.save(chunk, chunk_fname)


def save_datafiles_in_chunks(token_dirs, out_dir, chunk_size, url_file):
  """
  Reads the tokenized .story.json files corresponding to the urls
  listed in the url_file and writes them to a out_dir.
  Chunk will be named after the url list file name.

  :param token_dirs:   Array of strings. Path to the directories of tokenized stories.
  :param out_dir:     String. Path to the chunk_directory.
  :param chunk_size:  Uint. Size of a chunk.
  :param url_file:    String. Path to a file containing the list of
                      url to include.
  """

  print("Making bin file for URLs listed in %s..." % url_file)
  url_list = _read_text_file(url_file)
  url_hashes = _get_url_hashes(url_list)
  story_fnames = [s+".story.json" for s in url_hashes]
  num_stories = len(story_fnames)

  chunk = []
  chunk_nb = 0

  _, url_fname = os.path.split(url_file)
  set_name, _ = os.path.splitext(url_fname)
  print("Preparing chunks for %s set." % set_name)

  for idx, s in enumerate(story_fnames):
    if idx % 1000 == 0:
      print("Writing story %i of %i; %.2f percent done" %
            (idx, num_stories, float(idx)*100.0/float(num_stories)))

    if (idx % chunk_size == 0 and idx != 0) or idx + 1 == num_stories:
      write_chunk(out_dir, chunk, chunk_nb, set_name)
      chunk = []
      gc.collect()
      chunk_nb += 1

    # Look in the tokenized story dirs to find the .story file corresponding to this url
    story_file = _get_story_path(token_dirs, s)

    # Get the strings to write to binary: abstact is the abstraction-based summary and
    # oracles ids refer to sentences forming the target extractive summary.
    article_tokens_list, abstract_tokens_list, oracle_ids = gen_oracle_summary(
        story_file)
    _, story_fname = os.path.split(s)
    url_hash, _ = os.path.splitext(story_fname)
    b_data_dict = {"url_hash": url_hash, "article": article_tokens_list,
                   "abstract": abstract_tokens_list, "oracle_ids": oracle_ids}
    chunk.append(b_data_dict)

  print("Finished writing %s set chunks (%d)\n" % (set_name, chunk_nb))
