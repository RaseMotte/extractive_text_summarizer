from BaseModel import BaseModel
from rouge import Rouge

import os
import statistics as stats
import torch

ROUGE_TYPES = ["rouge-1", "rouge-2", "rouge-l"]
ROUGE_SUBTYPES = ["f", "p", "r"]

def _compute_mean(r_score_a, r_scores_b):
  """
  Computes the mean between two rouge scores.
  json example:
    {"rouge-1": {"f": 0.49411764217577864,
    "p": 0.5833333333333334,
    "r": 0.42857142857142855},
    "rouge-2": {"f": 0.23423422957552154,
    "p": 0.3170731707317073,
    "r": 0.18571428571428572},
    "rouge-l": {"f": 0.42751590030718895,
    "p": 0.5277777777777778,
    "r": 0.3877551020408163}}
  """
  res = dict()
  for rt in ROUGE_TYPES:
    rst_means = []
    for rst in ROUGE_SUBTYPES:
      rst_means.append(stats.mean([r_score_a[rt][rst], r_scores_b[rt][rst]]))
    res[rt] = dict(zip(ROUGE_SUBTYPES, rst_means))
  return res


def _removed_failed_trainsform(model_sum, h, art, target_sum):
  """
  Removed the failed transformed text from the dataset to write on disk.

  :param model_sum:   Model impleenting BaseModel abstract class
  :param h:           List of hashes.
  :param art:         List of article.
  :param target_sum:  List of target summary.
  """
  removed = 0
  model_sum.fail.sort()
  for aid in model_sum.fail:
    aid -= removed
    h.pop(aid)
    art.pop(aid)
    target_sum.pop(aid)
    removed += 1

def _save_pred_summaries(out_dir, h, pred_sum, batch_nb=None):
  """
  Write to disk the predicted summaries with the hash to the corresponding story.

  :param out_dir:   Path to the outpur directory.
  :param h:         List of hashes.
  :param pred_sum:  Summaries generated by the model.
  """
  res = [ dict(zip(["url_hash", "pred_sum"], [uh, s])) for uh, s in zip(h, pred_sum)]
  if batch_nb is not None:
    summaries_file = os.path.join(out_dir, 'summaries_%03d.bin' % batch_nb)
  else:
    summaries_file = os.path.join(out_dir, 'summaries.json')
  torch.save(res, summaries_file)


def _save_scores(out_dir, score_name, pred, ref, h, batch_nb=None):
  """
  Saving individual and average rouge scores to disk.

  :param out_dir:     Path to the outpur directory.
  :param score_name:  Name of the score. Used as file prefix.  
  :param ref:         List of hashes.
  :param pred:        Summaries generated.
  :param ref:         Reference to compute rouge against.
  :param batch_nb:    Optional. Used to number the file if set is used in batches.
  """
  rouge = Rouge()

  # Saving individual rouge scores to disk.
  r_scores = rouge.get_scores(pred, ref, avg=False)
  scores = [ dict(zip(["url_hash", "r_scores"], [uh, rs])) for uh, rs in zip(h, r_scores)]
  if batch_nb is not None:
    scores_file = os.path.join(out_dir, '%s_scores_%03d.json' % (score_name, batch_nb))
  else:
    scores_file = os.path.join(out_dir, '%s_scores.json' % score_name)
  torch.save(scores, scores_file)

  # Saving average rouge scores to disk.
  #avg_r_scores = rouge.get_scores(pred, ref, avg=True)
  #avg_scores_file = os.path.join(GENSIM_DATA_DIR, '%s_avg_scores_%03d.json' % (score_name, batch_nb))
  #torch.save({"avg_r_scores": avg_r_scores}, avg_scores_file)


def eval_model(model_sum: BaseModel, h, art, target_sum, out_dir, batch_nb=None):
  # Transforming article to extractive summaries.
  pred_sum = model_sum.transform(art)

  _removed_failed_trainsform(model_sum, h, art, target_sum)

  # Saving the result of the transform to disk.
  _save_pred_summaries(out_dir, h, pred_sum)

  # Saving to disk pred vs target rouge scores (individual and average)
  _save_scores(out_dir, 'pvst', pred_sum, target_sum, h, batch_nb)

  # Saving to disk pred vs art rouge scores (individual and average)
  _save_scores(out_dir, 'pvsa', pred_sum, art, h, batch_nb)

  # Saving to target vs art rouge scores (individual and average)
  _save_scores(out_dir, 'tvsa', target_sum, art, h, batch_nb)


def batch_eval_model(model_sum: BaseModel, h, art, target_sum, out_dir, batch_size=100):

  # global_mean_r_scores = None TODO

  batch_nb = 0
  start = 0
  end = batch_size

  while end <= len(h):
    # Mark % and time TODO
    print("Writing %i of %i; %.2f percent done" %
          (end, len(h), float(start)*100.0/float(len(h))))

    batch_h = h[start:end]
    batch_art = art[start:end]
    batch_target_sum = target_sum[start:end]
    #print(art, sep = '\n')
    #rouge_scores = gensim_sum.score(art, target_sum)

    eval_model(model_sum, batch_h, batch_art, batch_target_sum, out_dir, batch_nb)

    _removed_failed_trainsform(model_sum, h, art, target_sum)
    model_sum.fail = []

    batch_nb += 1
    start = end
    end += batch_size
    if end > len(h):
      end = len(h)
    if end == start:
      break
  print("Done")