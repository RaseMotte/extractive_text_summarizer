{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility fonctions to prepare the dmoz html dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%run web-summary/Process_Summary.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global path (datasets and intermediate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH=\"data/crawl-dmoz-fr-100000-html/\"\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH=\"data/dmoz-html-intermediate/\"\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(parsed_doc):\n",
    "    s = parsed_doc.find(\"link\", {\"rel\" : \"canonical\"})\n",
    "    url = None\n",
    "    if s is not None:\n",
    "        regex = re.search(\"href=\\\"https?://([^\\\"]+)\\\"\", str(s))\n",
    "        if regex:\n",
    "            url = regex.group(1)\n",
    "#           urls.append(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold summary load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_dmoz_html(bin_path=\"data/data.p\"):\n",
    "    gold_sum_dict = pickle.load(open(bin_path, 'rb'))\n",
    "    # Remove entries (url keys) with empty description.\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    gold_sum_dict = { url: gold for url, gold in gold_sum_dict.items() if gold != '' and gold is not None}\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    return gold_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document parsing\n",
    "\n",
    "### Parse html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_doc_dmoz_html(doc_folder=DATASET_PATH, sampling=1, part_id=None):\n",
    "    \"\"\"\n",
    "    Parse the html dmoz documents. Dataset is split in part, holding multiple web pages.\n",
    "    Reads each part and parse the pages assuming they are delimited by <html> ... </html> tags.\n",
    "    \n",
    "    :param doc_folder:   Path to the dataset directory holding html parts.\n",
    "    \n",
    "    :return:   Array of html web pages (strings).\n",
    "    \"\"\"\n",
    "\n",
    "    files = [file for file in os.listdir(doc_folder) if bool(re.match(r'part-[0-9]+', file))]\n",
    "    html_list = []\n",
    "        \n",
    "    # Compute nb part to keep regarding the sampling param.\n",
    "    # We assume parts are approximately of the same sizes.\n",
    "    tot = len(files)\n",
    "    perc = sampling * len(files)\n",
    "    print(\"[DMOZ HTML][DOC PARSE] Loading %d / %d parts\" % (perc, tot))\n",
    "    \n",
    "    for file_name in files:\n",
    "        if perc <= 0:\n",
    "            break\n",
    "            \n",
    "        # For each part, parse html pages\n",
    "        filepath = os.path.join(doc_folder, file_name)\n",
    "        print(\"[DMOZ HTML][DOC PARSE] Parsing %s\" % filepath)\n",
    "        file = open(filepath, 'r', encoding='utf-8')\n",
    "        html_list += re.findall(r'<html[^>]*>.*?<\\/html>', file.read(), re.DOTALL)\n",
    "        \n",
    "        perc -= 1\n",
    "\n",
    "    print(\"[DMOZ HTML][DOC PARSE] Total of html page loaded %d\" % len(html_list))\n",
    "    return html_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse raw text from html and segments of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tags = [\"script\", \"a\"]\n",
    "#[\"div\", \"p\", \"body\", \"html\", \"table\", \"tr\", \"li\", \"ul\", \"td\"]\n",
    "\n",
    "interesting_tags = [\"h1\", \"title\", \"bold\", \"b\", \"strong\", \"i\", \"em\", \"mark\", \"small\"]\n",
    "#interesting_tags = [\"title\"]\n",
    "it_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_soup(soup_doc):\n",
    "    \"\"\"\n",
    "    Remove tags not holding any text information.\n",
    "    \"\"\"\n",
    "    #print_str_soup(soup_doc)\n",
    "    for t in stop_tags:\n",
    "        to_remove = soup_doc.find_all(t)\n",
    "        for t in to_remove:\n",
    "            t.extract()\n",
    "    #print_str_soup(soup_doc)\n",
    "\n",
    "def get_interesting_segments(soup_doc):\n",
    "    \"\"\"\n",
    "    Builds a dictionay mapping tags of interest to text segments.\n",
    "    \"\"\"\n",
    "    doc_segments = dict()\n",
    "    for t in interesting_tags:\n",
    "        tag_list = soup_doc.find_all(t)\n",
    "        doc_segments[t] = [s for tag in tag_list for s in tag.stripped_strings]\n",
    "    return doc_segments\n",
    "\n",
    "def extract_text_and_segments(soup_list):\n",
    "    \"\"\"\n",
    "    Extract from the documents the text (content). And the segments\n",
    "    surrounded by tags of interrest.\n",
    "    All other html information are discarded after this step.\n",
    "    \"\"\"\n",
    "    docs = defaultdict(dict)\n",
    "    segments = defaultdict(dict)\n",
    "    cpt = 0\n",
    "    for soup_doc in soup_list:\n",
    "        if cpt == 2:\n",
    "            break\n",
    "        url = get_url(soup_doc)\n",
    "        clean_soup(soup_doc)\n",
    "        segments[url] = get_interesting_segments(soup_doc)\n",
    "        docs[url] = soup_doc.stripped_strings\n",
    "        #print(\"===========================================\")\n",
    "        cpt += 1\n",
    "    return docs, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the text and segments\n",
    "\n",
    "* lemmer\n",
    "* tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self):\n",
    "        pass\n",
    "    \n",
    "class RemoveStopWords():\n",
    "    def __init__(self, method, language):\n",
    "        if method == 'spacy':\n",
    "            sp = get_spacy_model(language)\n",
    "            spacy_model = spacy.lang.fr if language == \"french\" else spacy.lang.en\n",
    "            self.stop_words = spacy_model.stop_words.STOP_WORDS\n",
    "        elif method == 'nltk':\n",
    "            self.stop_words = nltk.corpus.stopwords.words(language)\n",
    "        else:\n",
    "            print(\"StopWords method not accepted: %s\" % self.method)\n",
    "        assert(self.stop_words is not None)\n",
    "        \n",
    "    def __call__(self, docs):\n",
    "        #print(self.stop_words)\n",
    "        docs_res = {}\n",
    "        for key, sents in docs.items():\n",
    "            docs_res[key] = []\n",
    "            for sent in sents:\n",
    "                sent_tmp = [w for w in sent if not w.is_stop]\n",
    "                docs_res[key].append(sent_tmp)\n",
    "        return docs_res\n",
    "    \n",
    "class Lemmer():\n",
    "    def __init__(self, method='spacy', language=\"french\"):\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "    \n",
    "    def __call__(self, docs):\n",
    "        \"\"\"\n",
    "        Bla.\n",
    "        \"\"\"\n",
    "        if self.method == \"spacy\":\n",
    "            lemmed_docs = spacy_lemmer(docs, self.language)\n",
    "        else:\n",
    "            print(\"Lemmer method not accepted: %s\" % self.method)\n",
    "        return lemmed_docs\n",
    "            \n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self, method='spacy', language=\"french\", len_sen=10, over=4):\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "        self.len_sen = len_sen\n",
    "        self.over = over\n",
    "\n",
    "    def __call__(self, docs):\n",
    "        \"\"\"\n",
    "        Tokenize documents.\n",
    "        \"\"\"\n",
    "        #Sentence Tokenization of the corpus\n",
    "        if self.method == 'nltk':\n",
    "            tokenized_docs = tokenizer_cleaner(docs)\n",
    "        elif self.method == 'brutal':\n",
    "            tokenized_docs = brutal_tokenizer(docs, self.len_sen)\n",
    "        elif self.method == 'overlap':\n",
    "            tokenized_docs = overlap_tokenizer(docs, self.len_sen, self.over)\n",
    "        elif self.method == 'spacy':\n",
    "            tokenized_docs = spacy_tokenizer(docs, self.language)\n",
    "        else :\n",
    "            print(\"Tokenizer method not accepted: %s\" % self.method)\n",
    "        return tokenized_docs\n",
    "\n",
    "\n",
    "def preprocess_text_and_segments(docs, segments, preprocessors):\n",
    "    for prep in preprocessors:\n",
    "        docs = prep(docs)\n",
    "        for url, doc_segments in segments.items():\n",
    "            segments[url] = prep(doc_segments)\n",
    "    return docs, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply weighting on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weighting_doc_dmoz_html(docs, w):\n",
    "    \"\"\"\n",
    "    Annotate the html document with html tag weight matrix.\n",
    "    :param doc_list:\n",
    "    :param w:          Dictionary. Maps a html tag to its weight.\n",
    "    \"\"\"\n",
    "    cpt = 0\n",
    "    for soup_doc in soup_list:\n",
    "        build_text_doc(soup_doc, w)\n",
    "        print(\"===========================================\")\n",
    "        cpt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build overall dictionary\n",
    "\n",
    "Keeps track of document and its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overall_dmoz_html(docs, gold_sum_dict):\n",
    "    \"\"\"\n",
    "    Keep document for which an url is found in the gold summary dictionay.\n",
    "    \"\"\"\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_sum_dict.keys())}\n",
    "    print(len(gold_sum_dict.keys()), len(docs.keys()), len(overall.keys()))\n",
    "    # True if 100% of the dataset is used\n",
    "    # assert(len(gold_sum_dict.keys()) == len(overall.keys()))\n",
    "    return overall\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull it all together for the Resume_Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_dmoz_html(method='nltk', len_sen=10, over=4, sampling=1):\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                gold_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded 26107\n",
      "[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded 18272\n"
     ]
    }
   ],
   "source": [
    "# Load gold summaries\n",
    "gold_sum_dict = load_gold_dmoz_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DMOZ HTML][DOC PARSE] Loading 1 / 100 parts\n",
      "[DMOZ HTML][DOC PARSE] Parsing data/crawl-dmoz-fr-100000-html/part-00000\n",
      "[DMOZ HTML][DOC PARSE] Total of html page loaded 942\n"
     ]
    }
   ],
   "source": [
    "# Load html dmoz documents\n",
    "html_list = parse_doc_dmoz_html(sampling=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DMOZ HTML][SOUP PARSE] Parsing 942 docs with soup.\n"
     ]
    }
   ],
   "source": [
    "# Parse html dmoz documents and dump the parsed tree to bin file\n",
    "print(\"[DMOZ HTML][SOUP PARSE] Parsing %d docs with soup.\" % len(html_list))\n",
    "soup_list = [BeautifulSoup(html_doc) for html_doc in html_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, segments = extract_text_and_segments(soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'esj-lille.fr/': <generator object Tag.stripped_strings at 0x1395dbcf0>, 'www.universac.com/': <generator object Tag.stripped_strings at 0x139ce9e58>})\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'esj-lille.fr/': {'h1': [], 'title': ['Ecole supérieure de journalisme de Lille - ESJ Lille'], 'bold': [], 'b': [], 'strong': [], 'i': [], 'em': [], 'mark': [], 'small': []}, 'www.universac.com/': {'h1': [], 'title': ['Modèle sac, sacs à main tendances et collections, sacs 2011'], 'bold': [], 'b': [], 'strong': [], 'i': [], 'em': [], 'mark': [], 'small': []}})\n"
     ]
    }
   ],
   "source": [
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = [\n",
    "    Tokenizer(method=\"spacy\", language=\"french\")\n",
    "    , RemoveStopWords(method=\"spacy\", language=\"french\")\n",
    "    , Lemmer(method=\"spacy\", language=\"french\")\n",
    "]\n",
    "docs, segments = preprocess_text_and_segments(docs, segments, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'esj-lille.fr/': [['ecole', 'superieur', 'journalisme', 'lille', 'esj', 'lille'], ['ouverture', 'inscription', 'concours', '2019', 'reportee', 'lundi', '21', 'janvier', '2019'], ['vacance', 'scolaire', '2018', '2019', 'stage', 'journalisme', 'l', 'esj', 'lille', '11', '20', 'an'], ['decouvrer', 'cap', 'podcast', 'realiser', 'spe', 'web', '93', 'promotion', 'l', 'esj', 'lille'], ['etudiant', 'international', 'candidater', 'l', 'esj', 'lille'], ['lesj', 'lille', 'sengage', 'important', 'projet', 'deducation', 'media', 'france'], ['ancien', 'promotion', 'l', 'oublier', 'groom', 'receptionnist', 'videur', 'cendrier', 'lutetia'], ['l', 'esj', 'lille', 'remporter', 'prix', 'regional', 'trophee', 'association', '2018', 'fondation'], ['7', 'janvier', 'prochain', 'l', 'ecole', 'superieur', 'journalisme', 'lille', 'partenariat', 'l', 'unesco'], ['dirigee', 'bertrand', 'laber', 'nouveau', 'serie', 'conjugu', 'dorenaver', 'partie', 'debat', 'largement'], ['lheur', 'defiance', 'journaliste', 'cesser', 'progresser'], ['97', 'etudiant', 'promo', '1', 'l', 'academie', 'esj', 'lille', 'valider', '1', 'annee', 'licence'], ['travail', 'detudiant'], ['chance', 'partie', 'competence', 'journalistique', 'mois', 'mai', 'etudiant', '93', 'promotion', 'lesj'], ['2018', 'etudiant', '93', 'promotion', 'lesj', 'lill', 'sinteressent', 'brexit', 'an', 'vote', 'britannique', 'faveur'], ['60', 'etudiant', 'deuxieme', 'anne', 'master', 'organiser', 'veritabl', 'redaction', 'transverse', 'tran', 'media', 'semaine'], ['gagner'], ['devoir', 'rester', 'quune', 'margot', 'pyckaert', 'phr23', 'remporter', 'mardi'], ['mousquetaire', 'promo', 'js25', 'mouch'], ['jimmy', 'leye', 'benjamin', 'robert', '25', 'promotion'], ['lundi', '24', 'septembre', 'lesj', 'lille', 'organiser', 'vernissage', 'lexposition', 'levacuation', 'letudiante', 'clair', 'duhamel', '93', 'promotion', 'master', 'generalist'], ['levacuation', 'retrac'], ['info'], ['lecole', 'superieure', 'journalisme', 'lille', 'proposer', 'formation', 'devenir', 'journaliste', 'tel'], ['lecole', 'superieure', 'journalisme', 'lille', 'organise', 'anne', 'rencontre', 'leducation', 'medier', 'rendez', 'reuni', 'professionnel', 'linformation'], ['preparer', 'passer', 'concours', 'grand', 'ecole', 'journalisme'], ['avoir', 'besoin', 'demulation', 'groupe', 'doper', 'methodologie'], ['lesj', 'lill'], ['tweet', 'esj', 'lille'], ['phenomene', 'mode', 'avenir'], ['audio', 'element', 'reponse', 'invite'], ['conference', 'journalisme', 'podcast', 'aujourdhui', 'midi', 'l'], ['representer', 'bi'], ['l'], ['media', 'face', 'defi'], ['bfm', 'cyril', 'auffret', 'tf1', 'flor'], ['facebook', 'esj', 'lille'], ['cadre', '10', 'an', 'prepa', 'egalite'], ['lesj', 'lill', 'remporter', 'prix', 'regional', 'trophee'], ['retrouver', 'lesj', 'lill', 'dinfo', 'formation'], ['tweet', 'docpresseesj'], ['conversation', 'pascal', 'colisson', 'neter', 'faute', 'ecole', 'journalisme'], ['nicaraguer', 'repression', 's', 'intensifier', 'media'], ['how', 'the', 'financial', 'time', 'i', 'building', 'brand', 'loyalty', 'among', 'young', 'readers'], ['droit', 'reserve', 'l', 'ecole', 'superieur', 'journalisme', 'lille'], ['poursuivre', 'navigation', 'site', 'accepter', 'l', 'utilisation', 'cookie', 'poursuite', 'navigation', 'site', 'valoir', 'consentement']], 'www.universac.com/': [['modele', 'sac', 'sac', 'main', 'tendance', 'collection', 'sac', '2011'], ['choisir', 'sac', 'main', 'proposer', 'faire', 'selection', 'fonction', 'silhouette', 'styliste', 'louis', 'labrecque', 'style', 'tendance', 'prendre', 'consideration', 'silhouette', 'developpe', 'approche', 'consister', 'associer', 'morphologie', 'lettre'], ['3', 'year', 'ago'], ['bientot', 'voir', 'collection', 'monogram', 'multicolore', 'louis', 'vuitton', 'maison', 'francais', 'decide', 'darreter', 'commercialisation', 'effectivement', 'collection', 'sac', 'main', 'fruit', 'dun', 'collaboration', 'griff', 'francaise', 'lartist', 'japonais', 'takashi', 'murakami', 'ete', 'lancee', '2003', 'collaboration', 'long'], ['3', 'year', 'ago'], ['monde', 'celebrite', 'piece', 'fetiche', 'porter', 'occasion', 'anne', 'sac', 'couleur', 'teinte', 'attir', 'lattention', 'amie', 'star', 'navez', 'didee', 'style', 'couleur', 'convier', 'sac'], ['3', 'year', 'ago'], ['acheteur', 'asiatique', 'saisir', 'luxueu', 'sac', 'main', 'rose', 'peau', 'crocodile', 'marque', 'hermer', 'dun', 'vente', 'enchere', 'sest', 'tenir', 'maison', 'christie', 'hong', 'kong', 'sac', 'vient', 'battre', 'record', 'vente', 'enchere', 'prix', 'deper', '15', 'estimation'], ['3', 'year', 'ago'], ['reput', 'finition', 'impeccable', 'herme', 'pretend', 'jamais', 'utiliser', 'ligne', 'montage', 'artisan', 'concoit', 'sac', 'main', 'fois', 'piece', 'cousu', 'individuel', 'main', 'creer', 'produit', 'finir', 'cest', 'quun', 'sac', 'main', 'rose', 'lumineux', 'issue', 'marque', 'herme'], ['3', 'year', 'ago'], ['ny', 'femme', 'monde', 'besoin', 'dun', 'sac', 'cest', 'raison', 'femme', 'tas', 'sac', 'garde', 'rob', 'continuer', 'acheter', 'devoir', 'mettre', 'limite', 'type', 'sac', 'femme', 'devoir'], ['3', 'year', 'ago'], ['saison', 'question', 'manquer', 'nouveau', 'tendance', 'vetement', 'chaussure', 'accessoire', 'y', 'anne', 'frange', 'sac', 'seau', 'vogue', 'rayon', 'sac', 'main', 'sac', 'seau', 'marqu', 'nouveau', 'saison'], ['3', 'year', 'ago'], ['incontournable', 'femme', 'sac', 'main', 'accessoire', 'indispensable', 'plutot', 'objet', 'desir', 'quon', 'jamais', 'passer', 'jamais', 'prendre', 'legere', 'selection', 'sac', 'main', 'mettre', 'valeur', 'silhouette', 'affirmer'], ['3', 'year', 'ago'], ['wilson', 'conception', 'cuir', 'developpe', 'fournir', 'ligne', 'mode', 'femme', 'homme', 'enfant', 'superviser', 'processus', 'texture', 'couleur', 'termin', 'vaste', 'experience', 'lindustrie', 'cuir', 'reputation', 'long', 'date', 'leader', 'conception', 'cuir'], ['3', 'year', 'ago'], ['voyage', 'continuer', 'michelle', 'williams', 'marque', 'luxe', 'louis', 'vuitton', 'effectivement', 'lactrice', 'lambassadrice', 'louis', 'vuitton', '2013', 'cest', 'ete', 'elue', 'egerie', 'nouveau', 'campagne', 'nouveau', 'campagne', 'marque', 'vouloir', 'mettre', 'valeur', 'nouveau', 'version', 'sac', 'volat', 'cliche', 'photographe'], ['3', 'year', 'ago']]}\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18272 2 2\n"
     ]
    }
   ],
   "source": [
    "overall = make_overall_dmoz_html(docs, gold_sum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'www.universac.com/': '', 'esj-lille.fr/': ''}\n"
     ]
    }
   ],
   "source": [
    "print(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_weighting_doc_dmoz_html(docs, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tag_frequency(html_list):\n",
    "    dic = {}\n",
    "\n",
    "    for html in html_list:\n",
    "        test = re.findall(r'<[^!\\</\\-\\?][^>/\\n:]*>', html, re.DOTALL)\n",
    "        for balise in test:\n",
    "            if \" \" in balise:\n",
    "                balise = balise.split(\" \")[0] + \">\"\n",
    "\n",
    "            if not balise in dic:\n",
    "                dic[balise] = 1\n",
    "            else:\n",
    "                dic[balise] += 1\n",
    "    return dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_proportions(dic):\n",
    "    mpl.rcParams['font.size'] = 9.0\n",
    "\n",
    "    #plt.figure(figsize=(30,20))\n",
    "    plt.pie(dic.values(), labels=dic.keys(), autopct='%1.1f%%')\n",
    "    plt.savefig('pie.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
