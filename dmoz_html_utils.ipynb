{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility fonctions for parsing the dmoz html dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global path (datasets and intermediate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH=\"data/crawl-dmoz-fr-100000-html/\"\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH=\"data/dmoz-html-intermediate/\"\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(parsed_doc):\n",
    "    s = parsed_doc.find(\"link\", {\"rel\" : \"canonical\"})\n",
    "    url = None\n",
    "    if s is not None:\n",
    "        regex = re.search(\"href=\\\"https?://([^\\\"]+)\\\"\", str(s))\n",
    "        if regex:\n",
    "            url = regex.group(1)\n",
    "#           urls.append(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold summary load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_dmoz_html(bin_path=\"data/data.p\"):\n",
    "    gold_sum_dict = pickle.load(open(bin_path, 'rb'))\n",
    "    # Remove entries (url keys) with empty description.\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    gold_sum_dict = { url: gold for url, gold in gold_sum_dict.items() if gold != '' and gold is not None}\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    return gold_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_part_dmoz_html(filepath):\n",
    "    \"\"\"\n",
    "    FIXME\n",
    "    \"\"\"\n",
    "    file = open(filepath, 'r', encoding='utf-8')\n",
    "    html_part_list = re.findall(r'<html[^>]*>.*?<\\/html>', file.read(), re.DOTALL)\n",
    "    return html_part_list\n",
    "\n",
    "stop_tags = [\"div\", \"p\", \"body\", \"html\", \"table\", \"tr\", \"li\", \"ul\", \"td\"]\n",
    "\n",
    "interesting_tags = [\"\"]\n",
    "\n",
    "def clean_soup(soup_doc):\n",
    "    soup_doc.replace_all()\n",
    "    \n",
    "\n",
    "\n",
    "def pars_doc_dmoz_html(doc_folder=DATASET_PATH):\n",
    "    \"\"\"\n",
    "    Parse the dmoz documents. Parses is done in 2 steps:\n",
    "        (1) Each part holds multiple web pages. First parse each pages from the part.\n",
    "        (2) Parse the html page with BeautifulSoup parser.\n",
    "        \n",
    "    The results of those 2 parsing are stored in intermediate files.\n",
    "    If the file already exists for the part, no parsing are executed.\n",
    "    \n",
    "    :param doc_folder:   Path to the dataset directory holding html parts.\n",
    "    \n",
    "    :return:   String. Path to the parsed documents.\n",
    "    \"\"\"\n",
    "\n",
    "    files = [file for file in os.listdir(doc_folder) if bool(re.match(r'part-[0-9]+', file))]\n",
    "\n",
    "    # Checks that directory to store soup files exists.\n",
    "    intermediate_soup_path = os.path.join(INTERMEDIATE_FILE_PATH, 'soup')\n",
    "    \n",
    "    if not os.path.exists(intermediate_soup_path):\n",
    "        os.makedirs(intermediate_soup_path)\n",
    "    \n",
    "    for file_name in files:\n",
    "        \n",
    "        # If the parsed part already exists, skip the parsing.\n",
    "        soup_part_path = os.path.join(intermediate_soup_path, file_name)\n",
    "        if os.path.exists(soup_part_path):\n",
    "            continue\n",
    "            \n",
    "        # For each part, parse html pages\n",
    "        print(\"[DMOZ HTML][DOC PARSE] Parsing %s\" % filepath)\n",
    "        filepath = os.path.join(doc_folder, file_name)\n",
    "        html_part_list = parse_part_dmoz_html(filepath)\n",
    "\n",
    "        # Parse html dmoz documents and dump the parsed tree to bin file\n",
    "        print(\"[DMOZ HTML][SOUP PARSE] Parsing %s\" % filepath)\n",
    "        soup_part_list = [BeautifulSoup(html_doc) for html_doc in html_part_list]\n",
    "        # pickle.dump(soup_part_list, open(soup_part_path, \"wb\"))\n",
    "        \n",
    "        # Clean useless tags\n",
    "        soup_part_list = [clean_soup(soup_doc) for soup_doc in soup_part_list]\n",
    "\n",
    "    # print(\"[DMOZ HTML][DOC LOAD] Total of html page loaded %d\" % len(html_list))\n",
    "    return intermediate_soup_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc_dmoz_soup(soup_folder, sampling=1):\n",
    "    \"\"\"\n",
    "    FIXME\n",
    "    \"\"\"\n",
    "    files = [file for file in os.listdir(soup_folder) if bool(re.match(r'part-[0-9]+', file))]\n",
    "    soup_list = []\n",
    "    \n",
    "    # Compute nb part to keep regarding the sampling param.\n",
    "    # We assume parts are approximately of the same sizes.\n",
    "    tot = len(files)\n",
    "    perc = sampling * len(files)\n",
    "    print(\"[DMOZ HTML][SOUP LOAD] Loading %d / %d parts\" % (perc, tot))\n",
    "    \n",
    "    for file_name in files:   \n",
    "        if perc <= 0:\n",
    "            break\n",
    "            \n",
    "        # For each part, load the parsed html\n",
    "        filepath = os.path.join(soup_folder, file_name)\n",
    "        soup_list += pickle.load(open(filepath, \"rb\"))\n",
    "        \n",
    "        perc -= 1\n",
    "            \n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overall_dmoz_html(soup_list, gold_sum_dict):\n",
    "    \"\"\"\n",
    "    Keep document for which an url is found in the gold summary dictionay.\n",
    "    \"\"\"\n",
    "    docs = { get_url(soup_doc) : soup_doc for soup_doc in soup_list}\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_sum_dict.keys())}\n",
    "    print(len(gold_sum_dict.keys()), len(docs.keys()), len(overall.keys()))\n",
    "    # True if 100% of the dataset is used\n",
    "    # assert(len(gold_sum_dict.keys()) == len(overall.keys()))\n",
    "    return docs, overall\n",
    "    # assert(len(html_list) == soup_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_doc(soup_doc, w):\n",
    "    print(soup_doc.get_text())\n",
    "    print(soup_doc.stripped_strings)\n",
    "    # List of string. Each string represent text contained within tag(s)\n",
    "    #text_doc = []\n",
    "    # tag(s) related to the text.\n",
    "    #tag_doc = []\n",
    "    # HTML tree traversal\n",
    "\n",
    "\n",
    "def apply_weighting_doc_dmoz_html(docs, w):\n",
    "    \"\"\"\n",
    "    Annotate the html document with html tag weight matrix.\n",
    "    :param doc_list:\n",
    "    :param w:          Dictionary. Maps a html tag to its weight.\n",
    "    \"\"\"\n",
    "    #docs = {url: build_text_doc(doc) for url, doc in docs.items()}\n",
    "    for soup_doc in soup_list:\n",
    "        text_doc = build_text_doc(soup_doc, w)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dmoz_html_corpus(docs, method='nltk', len_sen=10, over=4):\n",
    "    \"\"\"\n",
    "    Tokenize documents.\n",
    "    \"\"\"\n",
    "    #Sentence Tokenization of the corpus\n",
    "    if method == 'nltk' :\n",
    "        tokenized_docs = tokenizer_cleaner(docs)\n",
    "    elif method == 'brutal' :\n",
    "        tokenized_docs = brutal_tokenizer(docs, len_sen)\n",
    "    elif method == 'overlap' :\n",
    "        tokenized_docs = overlap_tokenizer(docs, len_sen, over)\n",
    "    else :\n",
    "        print(\"Method not accepted: %s\" % method)\n",
    "    return tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_dmoz_html(method='nltk', len_sen=10, over=4, sampling=1):\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                gold_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded 26107\n",
      "[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded 18272\n"
     ]
    }
   ],
   "source": [
    "# Load gold summaries\n",
    "gold_sum_dict = load_gold_dmoz_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DMOZ HTML][DOC LOAD] Parsing data/crawl-dmoz-fr-100000-html/part-00000\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-713fcf0c9d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load html dmoz documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msoup_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_doc_dmoz_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-d0e983387165>\u001b[0m in \u001b[0;36mparse_doc_dmoz_html\u001b[0;34m(doc_folder)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Parse html dmoz documents and dump the parsed tree to bin file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msoup_part_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhtml_doc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhtml_part_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup_part_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup_part_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# print(\"[DMOZ HTML][DOC LOAD] Total of html page loaded %d\" % len(html_list))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "# Load html dmoz documents\n",
    "soup_dir = parse_doc_dmoz_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_list = load_doc_dmoz_soup(sampling=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs, overall = make_overall_dmoz_html(soup_list, gold_sum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_weighting_doc_dmoz_html(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tag_frequency(html_list):\n",
    "    dic = {}\n",
    "\n",
    "    for html in html_list:\n",
    "        test = re.findall(r'<[^!\\</\\-\\?][^>/\\n:]*>', html, re.DOTALL)\n",
    "        for balise in test:\n",
    "            if \" \" in balise:\n",
    "                balise = balise.split(\" \")[0] + \">\"\n",
    "\n",
    "            if not balise in dic:\n",
    "                dic[balise] = 1\n",
    "            else:\n",
    "                dic[balise] += 1\n",
    "    return dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_proportions(dic):\n",
    "    mpl.rcParams['font.size'] = 9.0\n",
    "\n",
    "    #plt.figure(figsize=(30,20))\n",
    "    plt.pie(dic.values(), labels=dic.keys(), autopct='%1.1f%%')\n",
    "    plt.savefig('pie.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
