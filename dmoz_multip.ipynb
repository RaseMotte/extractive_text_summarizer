{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility fonctions to prepare the dmoz html dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum nb of process for the multiprocessing Pool.\n",
    "MAX_PROCESSES = 10 # 15 ok if I'm not fuzzing around with other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global path (datasets and intermediate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folder\n",
    "DATA_PATH = \"./data\"\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH= os.path.join(DATA_PATH, \"crawl-dmoz-fr-100000-html/\")\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH= os.path.join(DATA_PATH, \"dmoz-html-intermediate/\")\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(parsed_doc):\n",
    "    s = parsed_doc.find(\"link\", {\"rel\" : \"canonical\"})\n",
    "    url = None\n",
    "    if s is not None:\n",
    "        regex = re.search(\"href=\\\"https?://([^\\\"]+)\\\"\", str(s))\n",
    "        if regex:\n",
    "            url = regex.group(1)\n",
    "#           urls.append(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold summary load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_dmoz_html(bin_path=os.path.join(DATA_PATH, \"data.p\")):\n",
    "    gold_sum_dict = pickle.load(open(bin_path, 'rb'))\n",
    "    # Remove entries (url keys) with empty description.\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    gold_sum_dict = { url: gold for url, gold in gold_sum_dict.items() if gold != '' and gold is not None}\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    return gold_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document parsing\n",
    "\n",
    "### Parse html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_doc_dmoz_html(doc_folder=DATASET_PATH, sampling=1, part_id=None):\n",
    "    \"\"\"\n",
    "    Parse the html dmoz documents. Dataset is split in part, holding multiple web pages.\n",
    "    Reads each part and parse the pages assuming they are delimited by <html> ... </html> tags.\n",
    "    \n",
    "    :param doc_folder:   Path to the dataset directory holding html parts.\n",
    "    \n",
    "    :return:   Array of html web pages (strings).\n",
    "    \"\"\"\n",
    "\n",
    "    files = [file for file in os.listdir(doc_folder) if bool(re.match(r'part-[0-9]+', file))]\n",
    "    html_list = []\n",
    "        \n",
    "    # Compute nb part to keep regarding the sampling param.\n",
    "    # We assume parts are approximately of the same sizes.\n",
    "    tot = len(files)\n",
    "    perc = sampling * len(files)\n",
    "    print(\"[DMOZ HTML][DOC PARSE] Loading %d / %d parts\" % (perc, tot))\n",
    "    \n",
    "    for file_name in files:\n",
    "        if perc <= 0:\n",
    "            break\n",
    "            \n",
    "        # For each part, parse html pages\n",
    "        filepath = os.path.join(doc_folder, file_name)\n",
    "        print(\"[DMOZ HTML][DOC PARSE] Parsing %s\" % filepath)\n",
    "        file = open(filepath, 'r', encoding='utf-8')\n",
    "        html_list += re.findall(r'<html[^>]*>.*?<\\/html>', file.read(), re.DOTALL)\n",
    "        \n",
    "        perc -= 1\n",
    "\n",
    "    print(\"[DMOZ HTML][DOC PARSE] Total of html page loaded %d\" % len(html_list))\n",
    "    return html_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse raw text from html and segments of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tags = [\"script\", \"a\"]\n",
    "#[\"div\", \"p\", \"body\", \"html\", \"table\", \"tr\", \"li\", \"ul\", \"td\"]\n",
    "\n",
    "# Maybe remove strong\n",
    "interesting_tags = [\"h1\", \"title\", \"bold\", \"b\", \"i\", \"em\", \"mark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary of weights\n",
    "def get_html_bias():\n",
    "    html_bias = dict(zip(interesting_tags, [0.8] * len(interesting_tags)))\n",
    "    return html_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_soup(soup_doc):\n",
    "    \"\"\"\n",
    "    Remove tags not holding any text information.\n",
    "    \"\"\"\n",
    "    for t in stop_tags:\n",
    "        to_remove = soup_doc.find_all(t)\n",
    "        for t in to_remove:\n",
    "            t.extract()\n",
    "\n",
    "def get_interesting_segments(soup_doc):\n",
    "    \"\"\"\n",
    "    Builds a dictionay mapping tags of interest to text segments.\n",
    "    \"\"\"\n",
    "    doc_segments = dict()\n",
    "    for t in interesting_tags:\n",
    "        tag_list = soup_doc.find_all(t)\n",
    "        doc_segments[t] = [s for tag in tag_list for s in tag.stripped_strings]\n",
    "    return doc_segments\n",
    "\n",
    "def extract_text_and_segments(soup_list):\n",
    "    \"\"\"\n",
    "    Extract from the documents the text (content). And the segments\n",
    "    surrounded by tags of interrest.\n",
    "    All other html information are discarded after this step.\n",
    "    \"\"\"\n",
    "    docs = defaultdict(dict)\n",
    "    segments = defaultdict(dict)\n",
    "    for soup_doc in soup_list:\n",
    "        url = get_url(soup_doc)\n",
    "        if url is None:\n",
    "            continue\n",
    "        clean_soup(soup_doc)\n",
    "        segments[url] = get_interesting_segments(soup_doc)\n",
    "        docs[url] = [s for s in soup_doc.stripped_strings]\n",
    "    return docs, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = None\n",
    "def get_spacy_model(language):\n",
    "    global sp\n",
    "    if sp is not None:\n",
    "        return sp\n",
    "    if language == \"french\":\n",
    "        sp = spacy.load(\"fr_core_news_sm\")\n",
    "    elif language == \"english\":\n",
    "        sp = spacy.load('en_core_web_sm')\n",
    "    else:\n",
    "        print(\"Unknown spacy language %s\" % language)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeBlock\n",
    "\n",
    "* Generalize for all dataset a pipeline of processes.\n",
    "* Easier to multiprocess.\n",
    "\n",
    "=> Base \"abstract\" classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod #abstract base class\n",
    "\n",
    "class PipeBlock():\n",
    "    @abstractmethod\n",
    "    def __init__():\n",
    "        ...\n",
    "        \n",
    "    @abstractmethod\n",
    "    def __call__(self, doc):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalisation\n",
    "\n",
    "Define interface inorder to multiprocess the full pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_cleaner(sentence) : \n",
    "    #Method used to cleand a sentence of all diacritics all characters likes these.\n",
    "    return re.sub(r\"\\s+\",\" \", \n",
    "                  re.sub(r\"[^a-zA-Z0-9]\",\" \",\n",
    "                  unicodedata.normalize('NFKD', sentence).encode('ASCII', 'ignore').decode(\"utf-8\")\n",
    "                )\n",
    "             ).lower().strip()\n",
    "\n",
    "def tokenizer_cleaner(doc, language='french') :\n",
    "    # Method to create cleaned sentences with nltk\n",
    "    sentences = sent_tokenize(doc, language=language)\n",
    "    cleaned = []\n",
    "    for sen in sentences:\n",
    "        cleaned_sen = generic_cleaner(sen)\n",
    "        if len(cleaned_sen.split()) > 1:\n",
    "            cleaned.append(cleaned_sen)\n",
    "    #tok_doc = (sentences, cleaned)\n",
    "    tok_doc = cleaned\n",
    "    return tok_doc\n",
    "\n",
    "def brutal_tokenizer(doc, n) :\n",
    "    #Create sentences by cutting the document in portions of n words\n",
    "    toks = generic_cleaner(doc).split(\" \")\n",
    "    sentences = [\" \".join(toks[x*n:x*n+n]) for x in range(len(toks)//n)]\n",
    "    cleaned = sentences\n",
    "    #tok_doc = (sentences, cleaned)\n",
    "    tok_doc = cleaned\n",
    "    return tok_doc\n",
    "\n",
    "def overlap_tokenizer(doc, block_size, over_window) :\n",
    "    #Create sentences by cutting the document in portions of n words\n",
    "    toks = generic_cleaner(doc).split(\" \")\n",
    "    sentences = []\n",
    "    if len(toks) >= block_size :\n",
    "        sentences = [\" \".join(toks[x*over_window:x*over_window+block_size+1])\n",
    "                     for x in range( (len(toks)-block_size)//over_window+1)]\n",
    "    cleaned = sentences\n",
    "    #tok_doc = (sentences, cleaned,(block_size, over_window))\n",
    "    tok_doc = cleaned\n",
    "    return tok_doc\n",
    "\n",
    "def spacy_tokenizer(doc, language):\n",
    "    \"\"\"\n",
    "    Wrapper around the spacy tokenizer.\n",
    "    Adapts it to the corpus dictionay structure.\n",
    "    \"\"\"\n",
    "    sp = get_spacy_model(language)\n",
    "    cleaned_doc = [generic_cleaner(sent) for sent in doc]\n",
    "    cleaned_doc = [sp(sent) for sent in cleaned_doc]\n",
    "    cleaned_doc = [sent for sent in cleaned_doc if len(sent) > 1]\n",
    "    #sp_doc = [sp(sent) for sent in doc]\n",
    "    #tok_doc = (sp_doc, cleaned_doc)\n",
    "    tok_doc = cleaned_doc\n",
    "    return tok_doc\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self, method='spacy', language=\"french\", len_sen=10, over=4):\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "        self.len_sen = len_sen\n",
    "        self.over = over\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"\n",
    "        Tokenize documents.\n",
    "        \"\"\"\n",
    "        #Sentence Tokenization of the corpus\n",
    "        if self.method == 'nltk':\n",
    "            tokenized_doc = tokenizer_cleaner(doc)\n",
    "        elif self.method == 'brutal':\n",
    "            tokenized_doc = brutal_tokenizer(doc, self.len_sen)\n",
    "        elif self.method == 'overlap':\n",
    "            tokenized_doc = overlap_tokenizer(doc, self.len_sen, self.over)\n",
    "        elif self.method == 'spacy':\n",
    "            tokenized_doc = spacy_tokenizer(doc, self.language)\n",
    "        else :\n",
    "            print(\"Tokenizer method not accepted: %s\" % self.method)\n",
    "        return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopWords(PipeBlock):\n",
    "    def __init__(self, method, language):\n",
    "        if method == 'spacy':\n",
    "            sp = get_spacy_model(language)\n",
    "            spacy_model = spacy.lang.fr if language == \"french\" else spacy.lang.en\n",
    "            self.stop_words = spacy_model.stop_words.STOP_WORDS\n",
    "        elif method == 'nltk':\n",
    "            self.stop_words = nltk.corpus.stopwords.words(language)\n",
    "        else:\n",
    "            print(\"StopWords method not accepted: %s\" % self.method)\n",
    "        assert(self.stop_words is not None)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        doc_res = []\n",
    "        for sent in doc:\n",
    "            sent_tmp = [w.string for w in sent if not w.is_stop]\n",
    "            doc_res.append(\" \".join(sent_tmp))\n",
    "        return doc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmer(PipeBlock):\n",
    "    def __init__(self, method='spacy', language=\"french\"):\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "    \n",
    "    def __call__(self, docs):\n",
    "        \"\"\"\n",
    "        Bla.\n",
    "        \"\"\"\n",
    "        if self.method == \"spacy\":\n",
    "            lemmed_docs = spacy_lemmer(docs, self.language)\n",
    "        else:\n",
    "            print(\"Lemmer method not accepted: %s\" % self.method)\n",
    "        return lemmed_docs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Tfidf(PipeBlock):\n",
    "    def __init__(self, corpus, language):\n",
    "        stop_w = spacy.lang.fr.stop_words.STOP_WORDS if language == \"french\" else spacy.lang.en.stop_words.STOP_WORDS\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=stop_w)\n",
    "        self.vectorizer.fit(corpus)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        return self.vectorizer.transform(doc)\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return self.vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias on vector representation\n",
    "\n",
    "Define a class representing the bias for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab_doc_bias(doc_url, tag_map, bias_weight, vocab):\n",
    "    for tag, tokens in tag_map.items():\n",
    "        if (len(tokens) == 0):\n",
    "            continue\n",
    "        tokens = \" \".join(tokens).split()\n",
    "        doc_vocab_bias = { vocab[word] : bias_weight[tag] for word in tokens if word in vocab.values()}\n",
    "        return (doc_url, tag_map)\n",
    "\n",
    "def build_vocab_bias(vocab, doc_bias_terms, bias_weight, vocab_bias_file = None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab:           Dictionnary mapping a word to feature indices.\n",
    "    :param doc_bias_terms:  Dictionnay mapping a document key to a bias element key\n",
    "                            (ex html tag of interests). bias element key mapping to words,\n",
    "                            as they appear in the vocabulary.\n",
    "                            <=> segments quoi\n",
    "    :param bias_weight: Dictionnary mapping a bias element to its values (weight).\n",
    "    \"\"\"\n",
    "    if vocab_bias_file is not None and os.path.exists(vocab_bias_file):\n",
    "        return pickle.load(open(vocab_bias_file, 'rb'))\n",
    "    \n",
    "    vocab_bias = multip_for(_build_vocab_doc_bias, doc_bias_terms, (bias_weight, vocab))\n",
    "    \n",
    "    if vocab_bias_file is not None and not os.path.exists(vocab_bias_file):\n",
    "        pickle.dump(vocab_bias, open(vocab_bias_file, 'wb'))\n",
    "        \n",
    "    return dict(vocab_bias)\n",
    "    \n",
    "\n",
    "def _apply_vocab_doc_bias(doc_url, doc, vocab_bias):\n",
    "    \"\"\"\n",
    "    Given a vector representation of the document (each sentence is a vector of the vocab size), \n",
    "    adds to a word its bias weight.\n",
    "    \"\"\"\n",
    "    for word_id, weigth in vocab_bias[doc_url].items():\n",
    "        if doc[word_id] != 0:\n",
    "            doc[word_id] += weight\n",
    "    # Normalisation ??\n",
    "    return (doc_url, doc)\n",
    "\n",
    "def apply_vocab_bias(docs, vocab_bias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab:           Dictionnary mapping a word to feature indices.\n",
    "    :param doc_bias_terms:  Dictionnay mapping a document key to a bias element key\n",
    "                            (ex html tag of interests). bias element key mapping to words,\n",
    "                            as they appear in the vocabulary.\n",
    "                            <=> segments quoi\n",
    "    :param bias_weight: Dictionnary mapping a bias element to its values (weight).\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = multip_for(_apply_vocab_doc_bias, docs, [vocab_bias])\n",
    "        \n",
    "    return dict(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build overall dictionary\n",
    "\n",
    "Keeps track of document and its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overall_dmoz_html(docs, gold_sum_dict):\n",
    "    \"\"\"\n",
    "    Keep document for which an url is found in the gold summary dictionay.\n",
    "    \"\"\"\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_sum_dict.keys())}\n",
    "    print(len(gold_sum_dict.keys()), len(docs.keys()), len(overall.keys()))\n",
    "    # True if 100% of the dataset is used\n",
    "    # assert(len(gold_sum_dict.keys()) == len(overall.keys()))\n",
    "    return overall\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def _segments_wrapper(doc_url, segments, pipeline):\n",
    "    for name, transform in pipeline.items():\n",
    "        if name == \"vectorizer\":\n",
    "            break\n",
    "        for tag, seg in segments.items():\n",
    "            segments[tag] = transform(seg)\n",
    "    return (doc_url, segments)\n",
    "\n",
    "def _doc_wrapper(doc_url, doc, pipeline):\n",
    "    for name, transform in pipeline.items():\n",
    "        doc = transform(doc)\n",
    "    return (doc_url, doc)\n",
    "\n",
    "def run_pipeline(corpus, wrapper, pipeline=None, verbose=0):\n",
    "    \"\"\"\n",
    "    Loop over the dataset.\n",
    "    On each document, apply the transformation of each block in pipeline (in order).\n",
    "    \n",
    "    :param corpus:   Dictionary mapping a doc_key to its content (array of strings, 1 string = 1 sentence)\n",
    "    :param wrapper:  The wrapper is a function to adapt the behavior of the block to the dataset shape/structure.\n",
    "    :param pipeline: If none, u'r looking for troubles my friend.\n",
    "                     An array of actions to perform on a document of the corpus.\n",
    "                     Must follow the PipeBlock implementation.\n",
    "    \"\"\"\n",
    "    with Pool(MAX_PROCESSES) as ps:\n",
    "        corpus_res = ps.starmap(wrapper, [(doc_url, doc_sents, pipeline) for doc_url, doc_sents in corpus.items()])\n",
    "        corpus = dict(corpus_res)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def multip_for(func, it_dict, args):\n",
    "    \"\"\"\n",
    "    Loop over the dataset.\n",
    "    On each document, apply the transformation of each block in pipeline (in order).\n",
    "    \n",
    "    :param corpus:   Dictionary mapping a doc_key to its content (array of strings, 1 string = 1 sentence)\n",
    "    :param wrapper:  The wrapper is a function to adapt the behavior of the block to the dataset shape/structure.\n",
    "    :param pipeline: If none, u'r looking for troubles my friend.\n",
    "                     An array of actions to perform on a document of the corpus.\n",
    "                     Must follow the PipeBlock implementation.\n",
    "    \"\"\"\n",
    "    with Pool(MAX_PROCESSES) as ps:\n",
    "        res = ps.starmap(func, [(key, value, *args) for key, value in it_dict.items()])\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull it all together for the Resume_Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_dmoz_html(method='spacy', len_sen=10, over=4, sampling=1):\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                gold_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load gold summaries\n",
    "    gold_sum_dict = load_gold_dmoz_html()\n",
    "\n",
    "    # Load html dmoz documents\n",
    "    html_list = parse_doc_dmoz_html(sampling=sampling)\n",
    "\n",
    "    # Parse html dmoz documents and dump the parsed tree to bin file.\n",
    "    # Sadly, this task cannot be executed on multiple processes. The html\n",
    "    # tree generated is waaaaay too big to be serialized and send back to the\n",
    "    # main process. It might be worth to spend some time finding a trick\n",
    "    # solving this issue.\n",
    "    print(\"[DMOZ HTML][SOUP PARSE] Parsing %d docs with soup.\" % len(html_list))\n",
    "    soup_list = [BeautifulSoup(html_doc) for html_doc in html_list]\n",
    "    print(\"[DMOZ HTML][SOUP PARSE] Done\")\n",
    "    \n",
    "    \n",
    "    # extract document's text and segments of interests\n",
    "    # All the text is extracted from the document.\n",
    "    # TODO: See Readability.js to strengthen the quality / choice of the text extracted.\n",
    "    docs, segments = extract_text_and_segments(soup_list)\n",
    "    \n",
    "    # Build overall\n",
    "    print(\"[DMOZ HTML][OverALL] Starting\")\n",
    "    overall = make_overall_dmoz_html(docs, gold_sum_dict)\n",
    "    print(\"[DMOZ HTML][OverALL] Done\")\n",
    "    \n",
    "\n",
    "    # Apply preprocessing to text\n",
    "    # Corpus necessary for tfidf fit.\n",
    "    corpus = [\" \".join(doc) for doc in docs.values()]\n",
    "    print(\"[DMOZ HTML][PREP] Preprocessing pipeline\")\n",
    "    \n",
    "    # PCA on vocabulary to reduce its size ?\n",
    "    normalisation_pipeline = dict([\n",
    "        (\"tokenizer\", Tokenizer(method=\"spacy\", language=\"french\"))\n",
    "        ,(\"stopwords\", RemoveStopWords(method=\"spacy\", language=\"french\"))\n",
    "        #(\"lemmer\", Lemmer(method=\"spacy\", language=\"french\"))\n",
    "        ,(\"vectorizer\", Tfidf(corpus, language=\"french\"))\n",
    "    ])\n",
    "    \n",
    "    # Segment preprocessing\n",
    "    print(\"[DMOZ HTML][PREP] segments\")    \n",
    "    segments = run_pipeline(segments, _segments_wrapper, normalisation_pipeline)\n",
    "    \n",
    "    # Preprocess the document using tfidf biaised vocab.\n",
    "    # As tfidf return a normalized result, we don't have to worry\n",
    "    # to much on the impact of the bias on the vector representation.\n",
    "    print(\"[DMOZ HTML][PREP] docs\")    \n",
    "    docs = run_pipeline(docs, _doc_wrapper, normalisation_pipeline)\n",
    "    print(\"[DMOZ HTML][PREP] Done\")\n",
    "\n",
    "    # Build vocab bias matrix and apply the bias to the TFIDF vocab.\n",
    "    print(\"[DMOZ HTML][Bias] Building vocab bias for each document\")\n",
    "    vocab = normalisation_pipeline[\"vectorizer\"].get_vocabulary()\n",
    "    vocab_bias_file = os.path.join(INTERMEDIATE_FILE_PATH, \"vocab_bias_part-00068\")\n",
    "    # Normalization after adding bias\n",
    "    vocab_bias = build_vocab_bias(vocab, segments, get_html_bias(), vocab_bias_file)\n",
    "    print(\"[DMOZ HTML][Bias] Applying biais to documents\")\n",
    "    docs = apply_vocab_bias(docs, vocab_bias)\n",
    "    #docs = apply_bias(docs, vocab_bias)\n",
    "    print(\"[DMOZ HTML][Bias] Done\")\n",
    "    \n",
    "    # Return value following other dataset generation.\n",
    "    return docs, gold_sum_dict, overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs, gold_sum_dict, overall = generate_corpus_dmoz_html(sampling=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tag_frequency(html_list):\n",
    "    dic = {}\n",
    "\n",
    "    for html in html_list:\n",
    "        test = re.findall(r'<[^!\\</\\-\\?][^>/\\n:]*>', html, re.DOTALL)\n",
    "        for balise in test:\n",
    "            if \" \" in balise:\n",
    "                balise = balise.split(\" \")[0] + \">\"\n",
    "\n",
    "            if not balise in dic:\n",
    "                dic[balise] = 1\n",
    "            else:\n",
    "                dic[balise] += 1\n",
    "    return dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_proportions(dic):\n",
    "    mpl.rcParams['font.size'] = 9.0\n",
    "\n",
    "    #plt.figure(figsize=(30,20))\n",
    "    plt.pie(dic.values(), labels=dic.keys(), autopct='%1.1f%%')\n",
    "    plt.savefig('pie.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
