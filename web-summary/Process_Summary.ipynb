{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax as sax\n",
    "from xml.sax import SAXParseException\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHandler(sax.ContentHandler):\n",
    "    \"\"\"\n",
    "    Custom Handler for parsing the documents of the duc corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) :\n",
    "        self._activeParse = False\n",
    "        self._result = \"\"\n",
    "    \n",
    "    def startDocument(self):\n",
    "        pass\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name in {\"TEXT\", \"LEADPARA\"}:\n",
    "            self._activeParse = True\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name in {\"TEXT\", \"LEADPARA\"} :\n",
    "            self._activeParse = False\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._activeParse :\n",
    "            self._result += content.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryHandler(sax.ContentHandler):\n",
    "    \"\"\"\n",
    "    Custom Handler for parsing the summaries of the duc corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) :\n",
    "        self._activeParse = False\n",
    "        self._charBuffer = \"\"\n",
    "        self._active_doc = \"\"\n",
    "        self._result = []\n",
    "    \n",
    "    def startDocument(self):\n",
    "        pass\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == \"SUM\" :\n",
    "            self._activeParse = True\n",
    "            self._active_doc = attrs[\"DOCREF\"].strip(\" \")\n",
    "            self._charBuffer = \"\"\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == \"SUM\" :\n",
    "            self._activeParse = False\n",
    "            self._result.append((self._active_doc, self._charBuffer))\n",
    "            \n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._activeParse :\n",
    "            self._charBuffer += content.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file, handler):\n",
    "    \"\"\"\n",
    "    Reads and parse the given xml sax file.\n",
    "    Is used to secure exceptions of the parser\n",
    "    \n",
    "    :param file:       Path to the file to parse.\n",
    "    :param handler:    Handler for the xml sax parser.\n",
    "                       Must implement one of xml.sax.handler\n",
    "                       base classe. Handler holds return value.\n",
    "                       \n",
    "    :return:    The result value of the handler.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        sax.parse(open(file, encoding = 'utf-8'), handler)\n",
    "        return handler._result\n",
    "    except SAXParseException as e:\n",
    "        print(file,\"contains some errors\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_cleaner(sentence) : \n",
    "    #Method used to cleand a sentence of all diacritics all characters likes these.\n",
    "    return re.sub(r\"\\s+\",\" \",\n",
    "                  re.sub(r\"[^a-zA-Z0-9]\",\" \",\n",
    "                         unicodedata.normalize('NFKD', sentence).encode('ASCII', 'ignore').decode(\"utf-8\")\n",
    "                        )\n",
    "                 ).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_cleaner(corpus) :\n",
    "    # Method to create cleaned sentences with nltk\n",
    "    docs = {}\n",
    "    for key, sents in corpus.items() :\n",
    "        sentences = sent_tokenize(sents, language='french')\n",
    "        cleaned = []\n",
    "        for sen in sentences :\n",
    "            \n",
    "            cleaned_sen = generic_cleaner(sen)\n",
    "            if len(cleaned_sen.split()) > 1:\n",
    "                cleaned.append(cleaned_sen)\n",
    "        docs[key] = (sentences, cleaned)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brutal_tokenizer(corpus, n) :\n",
    "    #Create sentences by cutting the document in portions of n words\n",
    "    docs = {}\n",
    "    for key, sents in corpus.items() :\n",
    "        toks = generic_cleaner(sents).split(\" \")\n",
    "        sentences = [\" \".join(toks[x*n:x*n+n]) for x in range(len(toks)//n)]\n",
    "        cleaned = sentences\n",
    "        docs[key] = (sentences, cleaned)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_tokenizer(corpus, block_size, over_window) :\n",
    "    #Create sentences by cutting the document in portions of n words\n",
    "    docs = {}\n",
    "    for key, sents in corpus.items() :\n",
    "        toks = generic_cleaner(sents).split(\" \")\n",
    "        sentences = []\n",
    "        if len(toks) >= block_size :\n",
    "            sentences = [\" \".join(toks[x*over_window:x*over_window+block_size+1])\n",
    "                         for x in range( (len(toks)-block_size)//over_window+1)]\n",
    "        cleaned = sentences\n",
    "        docs[key] = (sentences, cleaned,(block_size, over_window))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_assembler(results, corpus, nb_words, over_window = 0) :\n",
    "    \"\"\"\n",
    "    Build the summary of a document with the result of the summarizer.\n",
    "    \n",
    "    Given the score for each sentences, selectes the top <nb_words> sentences\n",
    "    to build a summary.\n",
    "    \n",
    "    \n",
    "    :param results:     List of score per sentence in the document.\n",
    "                        Higher score denote a higher importance of the sentence.\n",
    "    :param corpus:      Dictionary mapping a document key to document content.\n",
    "    :param nb_words:    Number of words in the final summary.\n",
    "    :param over_window: The window over wich words are added to form a ngram.\n",
    "    \n",
    "    :return:   A list of sentences id picked as the top sentences for the summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the id of the sentences, ie, its position in the related document.\n",
    "    list_sent_id = [item[0] for item in sorted(enumerate(results), key=lambda x: x[1], reverse = True)]\n",
    "    \n",
    "    stops = set(stopwords.words('french'))\n",
    "    \n",
    "    # List of id forming the summary.\n",
    "    summary_sent_id = []\n",
    "    \n",
    "    if over_window:\n",
    "        words = set()\n",
    "        for sent_id in list_sent_id:\n",
    "            summary_sent_id.append(sent_id)\n",
    "            # ???\n",
    "            for i in range(sent_id * over_window, sent_id * over_window + len(corpus[sent_id].split())):\n",
    "                words.add(i)\n",
    "            if len(words) >= nb_words:\n",
    "                break\n",
    "    else:\n",
    "        for sent_id in list_sent_id:\n",
    "            summary_sent_id.append(sent_id)\n",
    "            if len( \" \".join([ \" \".join(set(corpus[i].split())-stops) for i in summary_sent_id]).split() ) >= nb_words:\n",
    "                break\n",
    "    return sorted(summary_sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_and_cut_at(sentence_list, limit, stops):\n",
    "    #Generate a sentence of limit words while cleaning stop words\n",
    "    return \" \".join(\" \".join([\" \".join(set(s.split())-stops) for s in sentence_list]).split()[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def wrapper(doc_key, summarizer, all_args) :\n",
    "    \"\"\"\n",
    "    Wrapper over the summarize method of the summarizers.\n",
    "    Passes arguments to the summarizer during multi-processing.\n",
    "    \n",
    "    :param doc_key:    Key of the document : document set id + document id.\n",
    "    \n",
    "    :returns:    A tuple : (doc_key summary_sent_id)\n",
    "                     doc_key: docset id + doc id\n",
    "                     summary_sent_id: list of sentences index of the document\n",
    "                     forming the summary for the given document.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # all_args[0] <=> preprocessed sentences of the document.\n",
    "    return (doc_key, summary_assembler(summarizer.summarize(all_args[0]), *all_args))\n",
    " \n",
    "\n",
    "def doc_summarizer(docs, summarizer, nb_words, over_window):\n",
    "    \"\"\"\n",
    "    Dispatch over a pool the summarization.\n",
    "    Each worker runs for a document (doc_key) a summary of\n",
    "    the document content (sents[1]) - array of cleaned sentences.\n",
    "    \n",
    "    \n",
    "    :param docs:           Tokenized corpus. Array of (???).\n",
    "    :param summarizer:     Summarizer process. See Summary_Processes directory.\n",
    "    :param nb_words:       Length of the summary, ie, the nb of word forming it.\n",
    "    :param over_window:    ???\n",
    "    \n",
    "    :return:    A dictionnay of predicted summaries.\n",
    "                Maps a document key (docset + doc id) to a list of\n",
    "                sentences index of the document forming the summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## INIT ##\n",
    "    corpus = [sen for sents in docs.values() for sen in sents[1]]\n",
    "    summarizer.preprocess(corpus)\n",
    "    print(\"Preprocess done\")\n",
    "    summary = {}\n",
    "    \n",
    "    # Multiprocessing for accelerate calculus but use more CPU\n",
    "    with Pool(1) as p:\n",
    "        list_summary = p.starmap(wrapper, [(doc_key , summarizer, (sents[1], nb_words, over_window)) \n",
    "                  for doc_key, sents in docs.items()])\n",
    "    \n",
    "    summary = {k : s for k,s in list_summary}\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(dirpath):\n",
    "    \"\"\"\n",
    "    Retrieve summaries from the given directory.\n",
    "    \n",
    "    Walks throught the given directory. For each sub directory,\n",
    "    opens, reads and parses a file mapping multiple textual summaries\n",
    "    to their source document. This file is named \"perdocs.correct\".\n",
    "    If the file doesn't exist, a message is print on stdin\n",
    "    with the path to the missing file.\n",
    "    \n",
    "    :Example:\n",
    "    \n",
    "    dirpath/\n",
    "        docset_0/\n",
    "            perdocs.correct\n",
    "        docset_1/\n",
    "            perdocs.correct\n",
    "        ...\n",
    "        docset_n/\n",
    "            perdocs.correct\n",
    "    \n",
    "    :param dirpath:    Path to the directory of summaries.\n",
    "                       Subdirectories reference a topic / document set (ex: d061 -> \"Gilbert's Hurricane\").\n",
    "    \n",
    "    :return:    summary_coprus: Dictionary mapping a string to a dictionary.\n",
    "                    A dictionary mapping a topic key and a document id to a list of sumaries.\n",
    "                    Summaries keys are letters referencing a summarizer.\n",
    "                 \n",
    "    :Example:\n",
    "    \n",
    "    {'d061j/AP880911-0016':\n",
    "        {'b': 'summary text',\n",
    "         'i': ''\n",
    "        },\n",
    "     'd061/P880912-0095':\n",
    "     ...\n",
    "    }\n",
    "    \"\"\"\n",
    "         \n",
    "    #Recupère les resumés des fichiers news\n",
    "    \n",
    "    walker = os.walk(dirpath)\n",
    "    _, subdirnames, _ = next(walker)\n",
    "\n",
    "    summary_corpus = defaultdict(lambda : {})\n",
    "    \n",
    "    for subdirname in subdirnames:\n",
    "        stream = None\n",
    "        try :\n",
    "            subdirpath = os.path.join(dirpath, subdirname)\n",
    "            index_path = os.path.join(subdirpath, \"perdocs.correct\")\n",
    "            stream = parse_file(index_path, SummaryHandler())\n",
    "        except FileNotFoundError : \n",
    "            print(\"%s does not exist\" % index_path)\n",
    "        if stream :\n",
    "            for (doc_id, sum_text) in stream:\n",
    "                summary_corpus[subdirname[:-1] + \"/\" + doc_id][subdirname[-1]] = sum_text\n",
    "    return summary_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_doc_linker(summaries, docs) :\n",
    "    \"\"\"\n",
    "    Associate \n",
    "    \n",
    "    :param summaries:    Dictionary mapping document set id and document id to multiple\n",
    "                         summaries (ex : output of get_summaries).\n",
    "    \n",
    "    :param docs:         Dictionary mapping document set id and document id to an\n",
    "                         array of sentences.\n",
    "    \n",
    "    \n",
    "    ..sealso:: get_summaries\n",
    "    \"\"\"\n",
    "    #Associe le corpus de news avec les fichiers news.\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit([sen for sents in docs.values() for sen in sents[1] ])\n",
    "    overall = defaultdict(lambda : {})\n",
    "    \n",
    "    for ref in summaries:\n",
    "        try :\n",
    "            X = tfidf_vectorizer.transform(docs[ref][1])\n",
    "            for anot in summaries[ref] :\n",
    "                gold = list(tokenizer_cleaner({ref : summaries[ref][anot]}).values())[0][1]\n",
    "                Y = tfidf_vectorizer.transform(gold)\n",
    "                M = cos_sim(X,Y)\n",
    "                overall[ref][anot] = np.argmax(M, axis = 0)\n",
    "        except KeyError as e :\n",
    "            print(\"File not parsed :\",e)\n",
    "    \n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from IPython.display import clear_output\n",
    "def html_gen(docs, resumes) :\n",
    "    #Generateur de pages html pour y afficher le résumé extrait\n",
    "    for doc in set(docs.keys()).intersection(set(resumes[0].keys())) :\n",
    "        \n",
    "        s = '<h2>'+doc+'</h2><p style=\"text-align:justify\">'\n",
    "        m1, m2, m3 = 0,0,0\n",
    "        \n",
    "        for i in range(len(docs[doc][0])) :\n",
    "            k = len(resumes)\n",
    "            m1 = 160 * (i in resumes[0][doc])\n",
    "            \n",
    "            if k-1 :\n",
    "                m2 = 160 * (i in resumes[1][doc])\n",
    "                \n",
    "                if k-2 :\n",
    "                    m3 = 160 * (i in resumes[2][doc])\n",
    "            \n",
    "            s += ('<span style=\"color:rgb('+str(m1)+','+str(m2)+','+str(m3)+')'\n",
    "                +';background-color:rgb('+str(255)+',255,'+str(255)+')\">'\n",
    "                + docs[doc][0][i]\n",
    "                +\" </span>\")\n",
    "        \n",
    "        display(HTML(s+\"</p>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
