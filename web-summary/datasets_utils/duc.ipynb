{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for corpus DUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [ ] vectorizer directly in pipeline\n",
    "- [ ] summary_doc_linker : pass vectorizer in param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.sax as sax\n",
    "from xml.sax import SAXParseException\n",
    "\n",
    "\n",
    "%run datasets_utils/preprocessors.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variable : paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folder\n",
    "DATA_PATH = \"../data\"\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH= os.path.join(DATA_PATH, \"DUC\")\n",
    "\n",
    "# Path to the documents\n",
    "DOCS_DIR = os.path.join(DATASET_PATH, \"docs\")\n",
    "\n",
    "# Path to the summaries\n",
    "SUMMARIES_DIR = os.path.join(DATASET_PATH, \"summaries\")\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH= os.path.join(DATASET_PATH, \"duc-intermediate/\")\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and parse the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHandler(sax.ContentHandler):\n",
    "    \"\"\"\n",
    "    Custom Handler for parsing the documents of the duc corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) :\n",
    "        self._activeParse = False\n",
    "        self._result = \"\"\n",
    "    \n",
    "    def startDocument(self):\n",
    "        pass\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name in {\"TEXT\", \"LEADPARA\"}:\n",
    "            self._activeParse = True\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name in {\"TEXT\", \"LEADPARA\"} :\n",
    "            self._activeParse = False\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._activeParse :\n",
    "            self._result += content.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file, handler):\n",
    "    \"\"\"\n",
    "    Reads and parse the given xml sax file.\n",
    "    Is used to secure exceptions of the parser\n",
    "    \n",
    "    :param file:       Path to the file to parse.\n",
    "    :param handler:    Handler for the xml sax parser.\n",
    "                       Must implement one of xml.sax.handler\n",
    "                       base classe. Handler holds return value.\n",
    "                       \n",
    "    :return:    The result value of the handler.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        sax.parse(open(file, encoding = 'utf-8'), handler)\n",
    "        return handler._result\n",
    "    except SAXParseException as e:\n",
    "        print(file,\"contains some errors\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and parse the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryHandler(sax.ContentHandler):\n",
    "    \"\"\"\n",
    "    Custom Handler for parsing the summaries of the duc corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) :\n",
    "        self._activeParse = False\n",
    "        self._charBuffer = \"\"\n",
    "        self._active_doc = \"\"\n",
    "        self._result = []\n",
    "    \n",
    "    def startDocument(self):\n",
    "        pass\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == \"SUM\" :\n",
    "            self._activeParse = True\n",
    "            self._active_doc = attrs[\"DOCREF\"].strip(\" \")\n",
    "            self._charBuffer = \"\"\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == \"SUM\" :\n",
    "            self._activeParse = False\n",
    "            self._result.append((self._active_doc, self._charBuffer))\n",
    "            \n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._activeParse :\n",
    "            self._charBuffer += content.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(dirpath):\n",
    "    \"\"\"\n",
    "    Retrieve summaries from the given directory.\n",
    "    \n",
    "    Walks throught the given directory. For each sub directory,\n",
    "    opens, reads and parses a file mapping multiple textual summaries\n",
    "    to their source document. This file is named \"perdocs.correct\".\n",
    "    If the file doesn't exist, a message is print on stdin\n",
    "    with the path to the missing file.\n",
    "    \n",
    "    :Example:\n",
    "    \n",
    "    dirpath/\n",
    "        docset_0/\n",
    "            perdocs.correct\n",
    "        docset_1/\n",
    "            perdocs.correct\n",
    "        ...\n",
    "        docset_n/\n",
    "            perdocs.correct\n",
    "    \n",
    "    :param dirpath:    Path to the directory of summaries.\n",
    "                       Subdirectories reference a topic / document set (ex: d061 -> \"Gilbert's Hurricane\").\n",
    "    \n",
    "    :return:    summary_coprus: Dictionary mapping a string to a dictionary.\n",
    "                    A dictionary mapping a topic key and a document id to a list of sumaries.\n",
    "                    Summaries keys are letters referencing a summarizer.\n",
    "                 \n",
    "    :Example:\n",
    "    \n",
    "    {'d061j/AP880911-0016':\n",
    "        {'b': 'summary text',\n",
    "         'i': ''\n",
    "        },\n",
    "     'd061/P880912-0095':\n",
    "     ...\n",
    "    }\n",
    "    \"\"\"\n",
    "         \n",
    "    #Recupère les resumés des fichiers news\n",
    "    \n",
    "    walker = os.walk(dirpath)\n",
    "    _, subdirnames, _ = next(walker)\n",
    "\n",
    "    summary_corpus = defaultdict(lambda : {})\n",
    "    \n",
    "    for subdirname in subdirnames:\n",
    "        stream = None\n",
    "        try :\n",
    "            subdirpath = os.path.join(dirpath, subdirname)\n",
    "            index_path = os.path.join(subdirpath, \"perdocs.correct\")\n",
    "            stream = parse_file(index_path, SummaryHandler())\n",
    "        except FileNotFoundError : \n",
    "            print(\"%s does not exist\" % index_path)\n",
    "        if stream :\n",
    "            for (doc_id, sum_text) in stream:\n",
    "                summary_corpus[subdirname[:-1] + \"/\" + doc_id][subdirname[-1]] = sum_text\n",
    "    return summary_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_doc_linker(summaries, docs) :\n",
    "    \"\"\"\n",
    "    Associate \n",
    "    \n",
    "    :param summaries:    Dictionary mapping document set id and document id to multiple\n",
    "                         summaries (ex : output of get_summaries).\n",
    "    \n",
    "    :param docs:         Dictionary mapping document set id and document id to an\n",
    "                         array of sentences.\n",
    "    \n",
    "    \n",
    "    ..sealso:: get_summaries\n",
    "    \"\"\"\n",
    "    #Associe le corpus de news avec les fichiers news.\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit([sen for sents in docs.values() for sen in sents])\n",
    "    overall = defaultdict(lambda : {})\n",
    "    \n",
    "    for ref in summaries:\n",
    "        try :\n",
    "            X = tfidf_vectorizer.transform(docs[ref])\n",
    "            for anot in summaries[ref] :\n",
    "                gold = list(tokenizer_cleaner({ref : summaries[ref][anot]}).values())[0][1]\n",
    "                print(\"DEBUG\", \"SUMMARY DOC LINKER\", \"GOLD\", gold)\n",
    "                Y = tfidf_vectorizer.transform(gold)\n",
    "                M = cos_sim(X,Y)\n",
    "                overall[ref][anot] = np.argmax(M, axis = 0)\n",
    "        except KeyError as e :\n",
    "            print(\"File not parsed :\",e)\n",
    "    \n",
    "    return overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_duc(language, sampling = 1 ):\n",
    "    \"\"\"\n",
    "    Generate a corpus from the DUC dataset with documents and summaries.\n",
    "    \n",
    "    :param sampling:    float. Threshold. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset id + docid to a parsed and tokenized document.\n",
    "                      'd061j/AP880911-0016' -> list(list of sentences unprocessed, list of cleaned sentences)\n",
    "                golden_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset id + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"\n",
    "    # Used for generating the corpus from duc\n",
    "    ## Preprocess des documents\n",
    "    doc_corpus = {}\n",
    "    walker = os.walk(DOCS_DIR)\n",
    "    _ = next(walker)\n",
    "\n",
    "    #Read all documents in subfolders  of the original folder\n",
    "    for docset, _, docnames in walker :\n",
    "        for docname in docnames :\n",
    "            docpath = os.path.join(docset, docname)\n",
    "            parsed_doc = parse_file(docpath, TextHandler())\n",
    "            if parsed_doc :\n",
    "                if random.random() < sampling:\n",
    "                    doc_key = os.path.join(docset.split(\"/\")[-1], docname)\n",
    "                    doc_corpus[doc_key] = parsed_doc\n",
    "    print(\"Loading done\")\n",
    "    \n",
    "    #Sentence Tokenization of the corpus\n",
    "    #tokenizer = Tokenizer(language, method, len_sen, over, min_doc_len=3)\n",
    "    #tokenized_docs = run_pipeline(doc_corpus, _doc_wrapper, {\"tokenizer\": tokenizer})\n",
    "    \n",
    "    #Cleaning part\n",
    "    #tokenized_docs = {k : tokenized_docs[k] for k in tokenized_docs if len(tokenized_docs[k])> 3}\n",
    "    \n",
    "    #Generating summaries\n",
    "    gold_sum_dict = get_summaries(SUMMARIES_DIR)\n",
    "    #overall = summary_doc_linker(summary_corpus, tokenized_docs)\n",
    "    #gold_tokenized_summaries = {x : tokenizer_cleaner(summary_corpus[x]) for x in summary_corpus}\n",
    "    \n",
    "    return doc_corpus, gold_sum_dict#, overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
