{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility fonctions to prepare the dmoz html dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "import logging\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from readability import Document\n",
    "\n",
    "%run datasets_utils/preprocessors.ipynb\n",
    "#%run preprocessors.ipynb\n",
    "\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    #clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text, end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global path (datasets and intermediate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    }
   ],
   "source": [
    "# Path to data folder\n",
    "DATA_PATH = \"/home/pfee/data\"\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH= os.path.join(DATA_PATH, \"crawl-dmoz-fr-100000-html\")\n",
    "\n",
    "# Path to the dataset\n",
    "DOCS_DIR = os.path.join(DATASET_PATH, \"docs\")\n",
    "\n",
    "# Path to the dataset\n",
    "SUM_DICT_PATH = os.path.join(DATASET_PATH, \"summaries/data.p\")\n",
    "\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH= os.path.join(DATASET_PATH, \"dmoz-html-intermediate/\")\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)\n",
    "    \n",
    "LOG_FILE = \"../logs/dmoz_html_debug.log\"\n",
    "if not os.path.exists(\"../logs/\"):\n",
    "    os.makedirs(\"../logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(parsed_doc):\n",
    "    s = parsed_doc.find(\"link\", {\"rel\" : \"canonical\"})\n",
    "    url = None\n",
    "    if s is not None:\n",
    "        regex = re.search(\"href=\\\"https?://([^\\\"]+)\\\"\", str(s))\n",
    "        if regex:\n",
    "            url = regex.group(1)\n",
    "#           urls.append(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold summary load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_dmoz_html(bin_path):\n",
    "    gold_sum_dict = pickle.load(open(bin_path, 'rb'))\n",
    "    # Remove entries (url keys) with empty description.\n",
    "    logging.debug(\"[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    gold_sum_dict = { url: gold for url, gold in gold_sum_dict.items() if gold != '' and gold is not None}\n",
    "    logging.debug(\"[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    return gold_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document parsing\n",
    "\n",
    "### Parse html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_doc_dmoz_html(doc_folder, sampling=1, part_id=None):\n",
    "    \"\"\"\n",
    "    Parse the html dmoz documents. Dataset is split in part, holding multiple web pages.\n",
    "    Reads each part and parse the pages assuming they are delimited by <html> ... </html> tags.\n",
    "    \n",
    "    :param doc_folder:   Path to the dataset directory holding html parts.\n",
    "    \n",
    "    :return:   Array of html web pages (strings).\n",
    "    \"\"\"\n",
    "\n",
    "    files = [file for file in os.listdir(doc_folder) if bool(re.match(r'part-[0-9]+', file))]\n",
    "    html_list = []\n",
    "        \n",
    "    # Compute nb part to keep regarding the sampling param.\n",
    "    # We assume parts are approximately of the same sizes.\n",
    "    tot = len(files)\n",
    "    perc = sampling * len(files)\n",
    "    logging.debug(\"[DMOZ HTML][DOC PARSE] Loading %d / %d parts\" % (perc, tot))\n",
    "    \n",
    "    for file_name in files:\n",
    "        if perc <= 0:\n",
    "            break\n",
    "            \n",
    "        # For each part, parse html pages\n",
    "        filepath = os.path.join(doc_folder, file_name)\n",
    "        logging.debug(\"[DMOZ HTML][DOC PARSE] Parsing %s\" % filepath)\n",
    "        file = open(filepath, 'r', encoding='utf-8')\n",
    "        html_list += re.findall(r'<html[^>]*>.*?<\\/html>', file.read(), re.DOTALL)\n",
    "        \n",
    "        perc -= 1\n",
    "\n",
    "    logging.debug(\"[DMOZ HTML][DOC PARSE] Total of html page loaded %d\" % len(html_list))\n",
    "    return html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_clean(html_list):\n",
    "    \"\"\"\n",
    "    Get the url of a html page with a regex. If none found, page is discarded.\n",
    "    Get text from html page with readability. If empty, page is discarded.\n",
    "    Get the html tree representation of the page.\n",
    "    \n",
    "    :param html_list:    List of strings. Each string represent a html page.\n",
    "    \n",
    "    :return:   A tuple of array.\n",
    "                First one is a list of html pages parsed with BeautifulSoup.\n",
    "                Second, is a list of urls matching the pages.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    urls = []\n",
    "    logging.disable(sys.maxsize)\n",
    "    for hdoc in html_list:\n",
    "        link = re.search(\"<link.*rel=\\\"canonical\\\".*>\", hdoc)\n",
    "        if link is not None:\n",
    "            url = re.search(\"href=\\\"https?://([^\\\"]+)\\\"\", link.group(0))\n",
    "            if url is not None:\n",
    "                url = url.group(1)\n",
    "                read_doc = Document(hdoc).summary()\n",
    "                bs_doc = BeautifulSoup(read_doc)\n",
    "                text = re.sub(r\"[\\n\\t\\s]*\", \"\", bs_doc.text)\n",
    "                if len(text):\n",
    "                    res.append(bs_doc)\n",
    "                    urls.append(url)\n",
    "    logging.disable(logging.NOTSET)\n",
    "    return res, urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse raw text from html and biased vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tags = [\"script\", \"a\"]\n",
    "#[\"div\", \"p\", \"body\", \"html\", \"table\", \"tr\", \"li\", \"ul\", \"td\"]\n",
    "\n",
    "# Maybe remove strong\n",
    "interesting_tags = [\"h1\", \"title\", \"bold\", \"b\", \"i\", \"em\", \"mark\"]\n",
    "\n",
    "wrapper_tags = [\"html\", \"body\", \"div\", \"figure\", \"figcaption\", \"img\", \"picture\", \"noscript\", \"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary of weights\n",
    "def get_html_bias():\n",
    "    html_bias = dict(zip(interesting_tags, [0.008] * len(interesting_tags)))\n",
    "    return html_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_soup(soup_doc):\n",
    "    \"\"\"\n",
    "    Remove tags not holding any text information.\n",
    "    \"\"\"\n",
    "    for t in stop_tags:\n",
    "        to_remove = soup_doc.find_all(t)\n",
    "        for t in to_remove:\n",
    "            t.extract()\n",
    "\n",
    "def get_biased_vocab(soup_doc):\n",
    "    \"\"\"\n",
    "    Builds a dictionay mapping tags of interest to text (vocab).\n",
    "    \"\"\"\n",
    "    doc_biased_vocab = dict()\n",
    "    for t in interesting_tags:\n",
    "        tag_list = soup_doc.find_all(t)\n",
    "        bias_vocab_tmp = [s for tag in tag_list for s in tag.stripped_strings]\n",
    "        doc_biased_vocab[t] = \" \".join(bias_vocab_tmp)\n",
    "    return doc_biased_vocab\n",
    "\n",
    "def extract_text_and_biased_vocab(soup_list, urls):\n",
    "    \"\"\"\n",
    "    Extract from the documents the text (content). And the\n",
    "    biased vocabulary surrounded by tags of interrest.\n",
    "    All other html information are discarded after this step.\n",
    "    \"\"\"\n",
    "    docs = defaultdict(dict)\n",
    "    biased_vocab = defaultdict(dict)\n",
    "    for soup_index, soup_doc in enumerate(soup_list):\n",
    "        url = urls[soup_index]\n",
    "        clean_soup(soup_doc)\n",
    "        biased_vocab[url] = get_biased_vocab(soup_doc)\n",
    "        doc_tmp = [s for s in soup_doc.stripped_strings]\n",
    "        docs[url] = \" \".join(doc_tmp)\n",
    "    return docs, biased_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build overall dictionary\n",
    "\n",
    "Keeps track of document and its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overall_dmoz_html(docs, gold_sum_dict):\n",
    "    \"\"\"\n",
    "    Keep document for which an url is found in the gold summary dictionay.\n",
    "    \"\"\"\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_sum_dict.keys())}\n",
    "    #logging.debug(len(gold_sum_dict.keys()), len(docs.keys()), len(overall.keys()))\n",
    "    # True if 100% of the dataset is used\n",
    "    # assert(len(gold_sum_dict.keys()) == len(overall.keys()))\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull it all together for the Resume_Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_dmoz_html(language, sampling = 1.0, verbose=10):\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                gold_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"\n",
    "    #logging.basicConfig(filename=LOG_FILE, level=logging.DEBUG)\n",
    "    # Load gold summaries\n",
    "    gold_sum_dict = load_gold_dmoz_html(SUM_DICT_PATH)\n",
    "\n",
    "    \"\"\"# Load html dmoz documents\n",
    "    html_list = parse_doc_dmoz_html(DOCS_DIR, sampling=sampling)\n",
    "    \n",
    "    # Parse html dmoz documents.\n",
    "    # FIXME : Try to multip now that html are smaller thanks to readability.js\n",
    "    # Sadly, this task cannot be executed on multiple processes. The html\n",
    "    # tree generated is waaaaay too big to be serialized and send back to the\n",
    "    # main process. It might be worth to spend some time finding a trick\n",
    "    # solving this issue.\n",
    "    logging.debug(\"[DMOZ HTML][READ] Readability parsing\")\n",
    "    soup_list, urls = readability_clean(html_list)\n",
    "    logging.debug(\"[DMOZ HTML][SOUP PARSE] Done\")\n",
    "    \n",
    "    # extract document's text and vocabulary bias\n",
    "    # All the text is extracted from the document.\n",
    "    docs, biased_vocab = extract_text_and_biased_vocab(html_list_read, urls)\"\"\"\n",
    "    \n",
    "    logging.debug(\"[DMOZ HTML][PREP] Starting deserialization\")\n",
    "    docs, biased_vocab = deserialize(\"fr\", corpus = \"dmoz-html\", sampling = sampling)\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Done\")\n",
    "    \n",
    "    \"\"\"logging.debug(\"[DMOZ HTML][PREP] Starting serialization\")\n",
    "    serialize_by_lang(docs, biased_vocab, sampling, corpus = \"dmoz-html\")\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Done\")\"\"\"\n",
    "        \n",
    "    # Apply preprocessing to text\n",
    "    # Corpus necessary for tfidf fit.\n",
    "    #corpus = [\" \".join(doc) for doc in docs.values()]\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Preprocessing pipeline\")\n",
    "    \n",
    "    # PCA on vocabulary to reduce size ?\n",
    "    \n",
    "    normalisation_pipeline = dict([\n",
    "        #(\"tokenizer\", Tokenizer(language, \"spacy\", len_sen, over, min_doc_len=0))\n",
    "        #,(\"stopwords\", RemoveStopWords(language, method))\n",
    "        (\"tokenizer\", Tokenizer(language, method, len_sen, over, min_doc_len=3))\n",
    "        #(\"lemmer\", Lemmer(method=\"spacy\", language=\"french\"))\n",
    "        #,(\"vectorizer\", Tfidf(corpus, language=\"french\"))\n",
    "    ])\n",
    "    \n",
    "    # Biased vocabulary preprocessing\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Biased vocab\")    \n",
    "    biased_vocab = run_pipeline(biased_vocab, _biased_vocab_wrapper, normalisation_pipeline)\n",
    "    \n",
    "    # Preprocess the document\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Docs\")    \n",
    "    docs = run_pipeline(docs, _doc_wrapper, normalisation_pipeline)\n",
    "    \n",
    "    # Preprocess the gold summaries\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Gold sum dict\")\n",
    "    normalisation_pipeline[\"tokenizer\"].min_doc_len = 1\n",
    "    gold_sum_dict = run_pipeline(gold_sum_dict, _doc_wrapper, normalisation_pipeline)\n",
    "    logging.debug(\"[DMOZ HTML][PREP] Done\")\n",
    "    \n",
    "    # Build overall\n",
    "    logging.debug(\"[DMOZ HTML][OverAll] Starting\")\n",
    "    overall = make_overall_dmoz_html(docs, gold_sum_dict)\n",
    "    logging.debug(\"[DMOZ HTML][OverAll] Done\")\n",
    "    docs, biased_vocab = extract_text_and_biased_vocab(soup_list, urls)\n",
    "    \n",
    "    # Return value following other dataset generation.\n",
    "    return docs, gold_sum_dict, biased_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tag_frequency(html_list):\n",
    "    dic = {}\n",
    "\n",
    "    for html in html_list:\n",
    "        test = re.findall(r'<[^!\\</\\-\\?][^>/\\n:]*>', html, re.DOTALL)\n",
    "        for balise in test:\n",
    "            if \" \" in balise:\n",
    "                balise = balise.split(\" \")[0] + \">\"\n",
    "\n",
    "            if not balise in dic:\n",
    "                dic[balise] = 1\n",
    "            else:\n",
    "                dic[balise] += 1\n",
    "    return dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_proportions(dic):\n",
    "    mpl.rcParams['font.size'] = 9.0\n",
    "\n",
    "    #plt.figure(figsize=(30,20))\n",
    "    plt.pie(dic.values(), labels=dic.keys(), autopct='%1.1f%%')\n",
    "    plt.savefig('pie.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
