{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMOZ utils\n",
    "\n",
    "# TODO\n",
    "\n",
    "- [ ] vectorizer directly in pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%run datasets_utils/preprocessors.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables (paths ect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folder\n",
    "DATA_PATH = \"../data\"\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH= os.path.join(DATA_PATH, \"dmoz\")\n",
    "\n",
    "# Path to the documents\n",
    "DOCS_DIR = os.path.join(DATASET_PATH, \"dmoz-fr-content.tsv\")\n",
    "\n",
    "# Path to the summaries\n",
    "SUMMARIES_DIR = os.path.join(DATASET_PATH, \"dmoz-fr-description\")\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH= os.path.join(DATASET_PATH, \"dmoz-intermediate/\")\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation corpus from the file\n",
    "def generate_corpus(language, method, len_sen = 10, over = 4, sampling = 1 ) :\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                golden_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"    \n",
    "    corpus = {}\n",
    "    with open(DOCS_DIR) as file :\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line :\n",
    "            key, data = line[:-1].split(\"\\t\")\n",
    "            if random.random() < sampling:\n",
    "                corpus[key] = data\n",
    "            line = file.readline()\n",
    "            i+=1\n",
    "\n",
    "    print(\"Loading done\")\n",
    "\n",
    "    #Sentence tokenizing part\n",
    "    tokenizer = Tokenizer(language, method, len_sen, over, min_doc_len=3)\n",
    "    {}\n",
    "    docs = run_pipeline(doc_corpus, _doc_wrapper, {\"tokenizer\": tokenizer})\n",
    "\n",
    "    #Cleaning part\n",
    "    #docs = {k : docs[k] for k in docs if len(docs[k])> 3}\n",
    "    \n",
    "\n",
    "    #Summaries generator\n",
    "    summary_corpus = {}\n",
    "    #stops = set(stopwords.words('french'))\n",
    "    stops = set()\n",
    "    with open(SUMMARIES_DIR) as file :\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line :\n",
    "            key, data = line[:-1].split(\"\\t\")\n",
    "            datac = \" \".join(set(data.split()) - stops)\n",
    "            summary_corpus[key] = { \"m\" : datac }\n",
    "            line = file.readline()\n",
    "            i+=1\n",
    "    \n",
    "    gold_tokenized_summaries = {x : tokenizer_cleaner(summary_corpus[x]) for x in summary_corpus }\n",
    "    gold_tokenized_summaries = {x : gold_tokenized_summaries[x] for x in gold_tokenized_summaries\n",
    "                               if all(len(gold_tokenized_summaries[x][a])>0 for a in gold_tokenized_summaries[x])\n",
    "                               }\n",
    "\n",
    "    #Linking summaries and corpus\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_tokenized_summaries.keys())}\n",
    "    \n",
    "    return docs, gold_tokenized_summaries, overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
