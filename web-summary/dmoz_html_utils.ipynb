{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility fonctions to prepare the dmoz html dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%run Process_Summary.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global path (datasets and intermediate data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "DATASET_PATH=\"../data/crawl-dmoz-fr-100000-html/\"\n",
    "\n",
    "# Path to directory in which intermediate data will be stored\n",
    "INTERMEDIATE_FILE_PATH=\"../data/dmoz-html-intermediate/\"\n",
    "\n",
    "if not os.path.exists(INTERMEDIATE_FILE_PATH):\n",
    "    os.makedirs(INTERMEDIATE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(parsed_doc):\n",
    "    s = parsed_doc.find(\"link\", {\"rel\" : \"canonical\"})\n",
    "    url = None\n",
    "    if s is not None:\n",
    "        regex = re.search(\"href=\\\"https?://([^\\\"]+)\\\"\", str(s))\n",
    "        if regex:\n",
    "            url = regex.group(1)\n",
    "#           urls.append(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold summary load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_dmoz_html(bin_path=\"../data/data.p\"):\n",
    "    gold_sum_dict = pickle.load(open(bin_path, 'rb'))\n",
    "    # Remove entries (url keys) with empty description.\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    gold_sum_dict = { url: gold for url, gold in gold_sum_dict.items() if gold != '' and gold is not None}\n",
    "    print(\"[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded %d\" % len(gold_sum_dict.keys()))\n",
    "    return gold_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document parsing\n",
    "\n",
    "### Parse html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_doc_dmoz_html(doc_folder=DATASET_PATH, sampling=1, part_id=None):\n",
    "    \"\"\"\n",
    "    Parse the html dmoz documents. Dataset is split in part, holding multiple web pages.\n",
    "    Reads each part and parse the pages assuming they are delimited by <html> ... </html> tags.\n",
    "    \n",
    "    :param doc_folder:   Path to the dataset directory holding html parts.\n",
    "    \n",
    "    :return:   Array of html web pages (strings).\n",
    "    \"\"\"\n",
    "\n",
    "    files = [file for file in os.listdir(doc_folder) if bool(re.match(r'part-[0-9]+', file))]\n",
    "    html_list = []\n",
    "        \n",
    "    # Compute nb part to keep regarding the sampling param.\n",
    "    # We assume parts are approximately of the same sizes.\n",
    "    tot = len(files)\n",
    "    perc = sampling * len(files)\n",
    "    print(\"[DMOZ HTML][DOC PARSE] Loading %d / %d parts\" % (perc, tot))\n",
    "    \n",
    "    for file_name in files:\n",
    "        if perc <= 0:\n",
    "            break\n",
    "            \n",
    "        # For each part, parse html pages\n",
    "        filepath = os.path.join(doc_folder, file_name)\n",
    "        print(\"[DMOZ HTML][DOC PARSE] Parsing %s\" % filepath)\n",
    "        file = open(filepath, 'r', encoding='utf-8')\n",
    "        html_list += re.findall(r'<html[^>]*>.*?<\\/html>', file.read(), re.DOTALL)\n",
    "        \n",
    "        perc -= 1\n",
    "\n",
    "    print(\"[DMOZ HTML][DOC PARSE] Total of html page loaded %d\" % len(html_list))\n",
    "    return html_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse raw text from html and segments of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tags = [\"script\", \"a\"]\n",
    "#[\"div\", \"p\", \"body\", \"html\", \"table\", \"tr\", \"li\", \"ul\", \"td\"]\n",
    "\n",
    "# Maybe remove strong\n",
    "interesting_tags = [\"h1\", \"title\", \"bold\", \"b\", \"i\", \"em\", \"mark\", \"small\"]\n",
    "it_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary of weights\n",
    "def get_html_bias():\n",
    "    html_bias = dict(zip(interesting_tags, [0.008] * len(interesting_tags)))\n",
    "    return html_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_soup(soup_doc):\n",
    "    \"\"\"\n",
    "    Remove tags not holding any text information.\n",
    "    \"\"\"\n",
    "    for t in stop_tags:\n",
    "        to_remove = soup_doc.find_all(t)\n",
    "        for t in to_remove:\n",
    "            t.extract()\n",
    "\n",
    "def get_interesting_segments(soup_doc):\n",
    "    \"\"\"\n",
    "    Builds a dictionay mapping tags of interest to text segments.\n",
    "    \"\"\"\n",
    "    doc_segments = dict()\n",
    "    for t in interesting_tags:\n",
    "        tag_list = soup_doc.find_all(t)\n",
    "        doc_segments[t] = [s for tag in tag_list for s in tag.stripped_strings]\n",
    "    return doc_segments\n",
    "\n",
    "def extract_text_and_segments(soup_list):\n",
    "    \"\"\"\n",
    "    Extract from the documents the text (content). And the segments\n",
    "    surrounded by tags of interrest.\n",
    "    All other html information are discarded after this step.\n",
    "    \"\"\"\n",
    "    docs = defaultdict(dict)\n",
    "    segments = defaultdict(dict)\n",
    "    for soup_doc in soup_list:\n",
    "        url = get_url(soup_doc)\n",
    "        if url is None:\n",
    "            continue\n",
    "        clean_soup(soup_doc)\n",
    "        segments[url] = get_interesting_segments(soup_doc)\n",
    "        docs[url] = [s for s in soup_doc.stripped_strings]\n",
    "    return docs, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the text and segments\n",
    "\n",
    "* lemmer\n",
    "* tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self):\n",
    "        pass\n",
    "    \n",
    "class RemoveStopWords():\n",
    "    def __init__(self, method, language):\n",
    "        self.method = method\n",
    "        if method == 'spacy':\n",
    "            sp = get_spacy_model(language)\n",
    "            spacy_model = spacy.lang.fr if language == \"french\" else spacy.lang.en\n",
    "            self.stop_words = spacy_model.stop_words.STOP_WORDS\n",
    "        else:\n",
    "            self.stop_words = nltk.corpus.stopwords.words(language)\n",
    "        assert(self.stop_words is not None)\n",
    "        \n",
    "    def __call__(self, docs):\n",
    "        if self.method == \"spacy\":\n",
    "            return self.spacy_stop_w(docs)\n",
    "        else:\n",
    "            return self.nltk_stop_w(docs)\n",
    "        \n",
    "    def spacy_stop_w(self, docs):\n",
    "        #print(self.stop_words)\n",
    "        docs_res = defaultdict(dict)\n",
    "        for key, sents in docs.items():\n",
    "            docs_res[key] = []\n",
    "            for sent in sents:\n",
    "                sent_tmp = [w.string for w in sent if not w.is_stop]\n",
    "                docs_res[key].append(\" \".join(sent_tmp))\n",
    "        return docs_res\n",
    "    \n",
    "    def nltk_stop_w(self, docs):\n",
    "        docs_res = defaultdict(dict)\n",
    "        for key, sents in docs.items():\n",
    "            docs_res[key] = []\n",
    "            for sent in sents:\n",
    "                sent_tmp = [w for w in sent.split() if w not in self.stop_words]\n",
    "                docs_res[key].append(\" \".join(sent_tmp))\n",
    "        return docs_res\n",
    "            \n",
    "    \n",
    "class Lemmer():\n",
    "    def __init__(self, method='spacy', language=\"french\"):\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "    \n",
    "    def __call__(self, docs):\n",
    "        \"\"\"\n",
    "        Bla.\n",
    "        \"\"\"\n",
    "        if self.method == \"spacy\":\n",
    "            lemmed_docs = spacy_lemmer(docs, self.language)\n",
    "        else:\n",
    "            print(\"Lemmer method not accepted: %s\" % self.method)\n",
    "        return lemmed_docs\n",
    "            \n",
    "\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self, method='spacy', language=\"french\", len_sen=10, over=4):\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "        self.len_sen = len_sen\n",
    "        self.over = over\n",
    "\n",
    "    def __call__(self, docs):\n",
    "        \"\"\"\n",
    "        Tokenize documents.\n",
    "        \n",
    "        :param doc:   array of strings.\n",
    "        \"\"\"\n",
    "        #Sentence Tokenization of the corpus\n",
    "        if self.method == 'nltk':\n",
    "            tokenized_docs = tokenizer_cleaner(docs)\n",
    "        elif self.method == 'brutal':\n",
    "            tokenized_docs = brutal_tokenizer(docs, self.len_sen)\n",
    "        elif self.method == 'overlap':\n",
    "            tokenized_docs = overlap_tokenizer(docs, self.len_sen, self.over)\n",
    "        elif self.method == 'spacy':\n",
    "            tokenized_docs = spacy_tokenizer(docs, self.language)\n",
    "        else :\n",
    "            print(\"Tokenizer method not accepted: %s\" % self.method)\n",
    "        return tokenized_docs\n",
    "\n",
    "def preprocess_text_and_segments(docs, segments, preprocessors):\n",
    "    for prep in preprocessors:\n",
    "        docs = prep(docs)\n",
    "        for url, doc_segments in segments.items():\n",
    "            segments[url] = prep(doc_segments)\n",
    "    return docs, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build overall dictionary\n",
    "\n",
    "Keeps track of document and its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overall_dmoz_html(docs, gold_sum_dict):\n",
    "    \"\"\"\n",
    "    Keep document for which an url is found in the gold summary dictionay.\n",
    "    \"\"\"\n",
    "    docs_keys = docs.keys()\n",
    "    gold_keys = gold_sum_dict.keys()\n",
    "    overall = {x : \"\" for x in set(docs_keys).intersection(gold_keys)}\n",
    "    print(len(gold_keys), len(docs_keys), len(overall.keys()))\n",
    "    # True if 100% of the dataset is used\n",
    "    # assert(len(gold_sum_dict.keys()) == len(overall.keys()))\n",
    "    return overall\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull it all together for the Resume_Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_dmoz_html(method='spacy', len_sen=10, over=4, sampling=1):\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                gold_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load gold summaries\n",
    "    gold_sum_dict = load_gold_dmoz_html()\n",
    "\n",
    "    # Load html dmoz documents\n",
    "    html_list = parse_doc_dmoz_html(sampling=0.01)\n",
    "\n",
    "    # Parse html dmoz documents and dump the parsed tree to bin file\n",
    "    print(\"[DMOZ HTML][SOUP PARSE] Parsing %d docs with soup.\" % len(html_list))\n",
    "    soup_list = [BeautifulSoup(html_doc) for html_doc in html_list]\n",
    "    print(\"[DMOZ HTML][SOUP PARSE] Done\")\n",
    "    \n",
    "    print(\"[DMOZ HTML][SEGMENTS] Starting\")\n",
    "    docs, segments = extract_text_and_segments(soup_list)\n",
    "    print(\"[DMOZ HTML][SEGMENTS] Done\")\n",
    "\n",
    "    print(\"[DMOZ HTML][Normalizer] Starting\")\n",
    "    preprocessors = [\n",
    "        Tokenizer(method=method, len_sen=len_sen, over=over, language=\"french\")\n",
    "        , RemoveStopWords(method, language=\"french\")\n",
    "        #, Lemmer(method=\"spacy\", language=\"french\")\n",
    "    ]\n",
    "    \n",
    "    docs, segments = preprocess_text_and_segments(docs, segments, preprocessors)\n",
    "    print(\"[DMOZ HTML][Normalizer] Done\")\n",
    "    \n",
    "    print(\"[DMOZ HTML][Overall] Starting\")\n",
    "    overall = make_overall_dmoz_html(docs, gold_sum_dict)\n",
    "    print(\"[DMOZ HTML][Overall] Done\")\n",
    "\n",
    "    return docs, gold_sum_dict, overall, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tag_frequency(html_list):\n",
    "    dic = {}\n",
    "\n",
    "    for html in html_list:\n",
    "        test = re.findall(r'<[^!\\</\\-\\?][^>/\\n:]*>', html, re.DOTALL)\n",
    "        for balise in test:\n",
    "            if \" \" in balise:\n",
    "                balise = balise.split(\" \")[0] + \">\"\n",
    "\n",
    "            if not balise in dic:\n",
    "                dic[balise] = 1\n",
    "            else:\n",
    "                dic[balise] += 1\n",
    "    return dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_proportions(dic):\n",
    "    mpl.rcParams['font.size'] = 9.0\n",
    "\n",
    "    #plt.figure(figsize=(30,20))\n",
    "    plt.pie(dic.values(), labels=dic.keys(), autopct='%1.1f%%')\n",
    "    plt.savefig('pie.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
