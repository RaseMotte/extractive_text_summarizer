{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizers Interface\n",
    "\n",
    "\n",
    "This file instantiates all general parameters linked to model testing (and run the tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et chargement des m√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%run Process_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/Generic_Summarizer.ipynb\n",
    "\n",
    "%run Summary_Processes/Baseline_Summay.ipynb\n",
    "%run Summary_Processes/Random_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TextRank_Jaccard_Summary.ipynb\n",
    "%run Summary_Processes/Embeddings_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TFIDF_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TextRank_TFIDF_Assym_Summary.ipynb\n",
    "\"\"\"\n",
    "%run Summary_Processes/Cluster_Summary.ipynb\n",
    "%run Summary_Processes/Cluster_First_Summary.ipynb\n",
    "\"\"\"\n",
    "%run Summary_Processes/Sumy_Summary.ipynb\n",
    "\n",
    "import os\n",
    "import rouge\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pprint import pprint as pp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_gen(docs, resumes) :\n",
    "    for doc in set(docs.keys()).intersection(set(resumes[0].keys())) :\n",
    "        \n",
    "        #Head string with name of the file\n",
    "        s = '<h2>'+doc+'</h2><p style=\"text-align:justify\">'\n",
    "        \n",
    "        #Color Modifiers\n",
    "        m1, m2, m3 = 0,0,0\n",
    "        \n",
    "        for i in range(len(docs[doc][0])) :\n",
    "            k = len(resumes)\n",
    "            \n",
    "            #Color assignement\n",
    "            m1 = 160 * (i in resumes[0][doc])\n",
    "            if k-1 :\n",
    "                m2 = 160 * (i in resumes[1][doc])\n",
    "                if k-2 :\n",
    "                    m3 = 160 * (i in resumes[2][doc])\n",
    "            \n",
    "            #Generating the sentence colored\n",
    "            s += ('<span style=\"color:rgb('+str(m1)+','+str(m2)+','+str(m3)+')'\n",
    "                +';background-color:rgb('+str(255)+',255,'+str(255)+')\">'\n",
    "                + docs[doc][0][i]\n",
    "                +\" </span>\")\n",
    "        \n",
    "        #Display of the sentence\n",
    "        display(HTML(s+\"</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_duc(method = 'nltk', len_sen = 10, over = 4, sampling = 1 ) :\n",
    "    \"\"\"\n",
    "    Generate a corpus from the DUC dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String. Reference a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     int. Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ???\n",
    "                        Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    float. Threshold. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset id + docid to a parsed and tokenized document.\n",
    "                      'd061j/AP880911-0016' -> list(list of sentences unprocessed, list of cleaned sentences)\n",
    "                golden_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset id + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"\n",
    "    # Used for generating the corpus from duc\n",
    "    ## Preprocess des documents\n",
    "    doc_folder = \"../wdl/resume/docs/\"\n",
    "    doc_corpus = {}\n",
    "    walker = os.walk(doc_folder)\n",
    "    _ = next(walker)\n",
    "\n",
    "    #Read all documents in subfolders  of the original folder\n",
    "    for docset, _, docnames in walker :\n",
    "        for docname in docnames :\n",
    "            docpath = os.path.join(docset, docname)\n",
    "            parsed_doc = parse_file(docpath, TextHandler())\n",
    "            if parsed_doc :\n",
    "                if random.random() < sampling:\n",
    "                    doc_key = os.path.join(docset.split(\"/\")[-1], docname)\n",
    "                    doc_corpus[doc_key] = parsed_doc\n",
    "    print(\"Loading done\")\n",
    "    \n",
    "    #Sentence Tokenization of the corpus\n",
    "    if method == 'nltk' :\n",
    "        tokenized_docs = tokenizer_cleaner(doc_corpus)\n",
    "    elif method == 'brutal' :\n",
    "        tokenized_docs = brutal_tokenizer(doc_corpus, len_sen)\n",
    "    elif method == 'overlap' :\n",
    "        tokenized_docs = overlap_tokenizer(doc_corpus, len_sen, over)\n",
    "    else :\n",
    "        print(\"Method not accepted: %s\" % method)\n",
    "    \n",
    "    #Cleaning part\n",
    "    tokenized_docs = {k : tokenized_docs[k] for k in tokenized_docs if len(tokenized_docs[k][1])> 3}\n",
    "    \n",
    "    #Generating summaries\n",
    "    summary_folder = \"../wdl/resume/summaries/\"\n",
    "    summary_corpus = get_summaries(summary_folder)\n",
    "    overall = summary_doc_linker(summary_corpus, tokenized_docs)\n",
    "    gold_tokenized_summaries = {x : tokenizer_cleaner(summary_corpus[x]) for x in summary_corpus}\n",
    "    return tokenized_docs, gold_tokenized_summaries, overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus dmoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation corpus from the file\n",
    "def generate_corpus(method = 'nltk', len_sen = 10, over = 4, sampling = 1 ) :\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                golden_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"    \n",
    "    corpus = {}\n",
    "    with open(\"../wdl/resume/dmoz-fr-content.tsv\") as file :\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line :\n",
    "            key, data = line[:-1].split(\"\\t\")\n",
    "            if random.random() < sampling:\n",
    "                corpus[key] = data\n",
    "            line = file.readline()\n",
    "            i+=1\n",
    "\n",
    "    print(\"Loading done\")\n",
    "\n",
    "    #Sentence tokenizing part\n",
    "    if method == 'nltk' :\n",
    "        docs = tokenizer_cleaner(corpus)\n",
    "    elif method == 'brutal' :\n",
    "        docs = brutal_tokenizer(corpus, len_sen)\n",
    "    elif method == 'overlap' :\n",
    "        docs = overlap_tokenizer(corpus, len_sen, over)\n",
    "    else :\n",
    "        print(\"Method not accepted\")\n",
    "\n",
    "    #Cleaning part\n",
    "    docs = {k : docs[k] for k in docs if len(docs[k][1])> 3}\n",
    "\n",
    "    #Summaries generator\n",
    "    summary_corpus = {}\n",
    "    #stops = set(stopwords.words('french'))\n",
    "    stops = set()\n",
    "    with open(\"../wdl/resume/dmoz-fr-description\") as file :\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line :\n",
    "            key, data = line[:-1].split(\"\\t\")\n",
    "            datac = \" \".join(set(data.split()) - stops)\n",
    "            summary_corpus[key] = { \"m\" : datac }\n",
    "            line = file.readline()\n",
    "            i+=1\n",
    "    \n",
    "    gold_tokenized_summaries = {x : tokenizer_cleaner(summary_corpus[x]) for x in summary_corpus }\n",
    "    gold_tokenized_summaries = {x : gold_tokenized_summaries[x] for x in gold_tokenized_summaries\n",
    "                               if all(len(gold_tokenized_summaries[x][a][1])>0 for a in gold_tokenized_summaries[x])\n",
    "                               \n",
    "                               }\n",
    "\n",
    "    #Linking summaries and corpus\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_tokenized_summaries.keys())}\n",
    "    \n",
    "    return docs, gold_tokenized_summaries, overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers Initialization\n",
    "\n",
    "Define the _option_ variable which a list of summarization processes to test.\n",
    "\n",
    "* Each summarization process is instantiaded with its options.\n",
    "* A summarization process is a model and its routines.\n",
    "* Models and their routines are defined in the [Summary_Processes folder](http://localhost:8888/tree/web-summary/Summary_Processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running summarizers\n"
     ]
    }
   ],
   "source": [
    "print(\"Running summarizers\")\n",
    "import math\n",
    "#List of all options \n",
    "def const_1(x):\n",
    "    return 1\n",
    "def const_2(x):\n",
    "    return 2\n",
    "emb_fol = \"../wdl/clustering/crawl-dmoz-fr/docWords/\"\n",
    "option = ([Baseline_process(),\n",
    "          Random_process(),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"tr\", lsanbcompfun = math.log, diag = \"none\"),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False,\n",
    "          #                                        method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          # Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4, ldanbcompfun = const_1, bias = 0.0),\n",
    "          #Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4,\n",
    "          #                   method = \"lsa\", ldanbcompfun = const_1),\n",
    "         ]\n",
    "         #[TextRank_Jaccard_process(1, 0.25*i, weighted = False, method = m, lsanbcompfun = const_1, diag = diag, bias = 0.5*bias)\n",
    "         #     for i in range(1) for diag in (\"none\", \"before\") for m in (\"tr\", \"lsa\") for bias in [3]] \n",
    "         #+ [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "         #                    emb_fol, a = 1, b = 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "         )\n",
    "\n",
    "\n",
    "#List of options for sumy\n",
    "folder = \"../wdl/resume/docs/\"\n",
    "summy_option = [ Sumy_process(mode, folder, \"french\", nb_sent = 2) for mode in [\"text_rank\"]]\n",
    "#, \"sum_basic\", \"kl\", \"luhn\", \"lsa\", \"lex_rank\"]]\n",
    "#summy_option = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an empty dictionary in wich median ROUGE L scores will be stored.\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-1'],)\n",
    "rouge_score = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers testing process\n",
    "\n",
    "* Load stop words (optional)\n",
    "* Cut the corpus (is this tokenization ?)\n",
    "* Generate corpuses\n",
    "* Mystic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####brutal-5####\n",
      "Loading done\n",
      "\n",
      "Baseline\n",
      "Preprocess done\n",
      "\n",
      "Time : 14.66s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.15749552231881891, 'p': 0.14805598310227971, 'r': 0.1801810550731936}\n",
      "\n",
      "Random_process nb0\n",
      "Preprocess done\n",
      "\n",
      "Time : 15.47s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1349138391377549, 'p': 0.12484951408966216, 'r': 0.15609418206485132}\n",
      "\n",
      "####brutal-10####\n",
      "Loading done\n",
      "\n",
      "Baseline\n",
      "Preprocess done\n",
      "\n",
      "Time : 10.42s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1593662881077148, 'p': 0.14721897632513664, 'r': 0.18563139214954505}\n",
      "\n",
      "Random_process nb0\n",
      "Preprocess done\n",
      "\n",
      "Time : 11.87s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1375510447918927, 'p': 0.12565368230566754, 'r': 0.16154320696684502}\n",
      "\n",
      "####brutal-15####\n",
      "Loading done\n",
      "\n",
      "Baseline\n",
      "Preprocess done\n",
      "\n",
      "Time : 10.83s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16257303338112758, 'p': 0.14784863270619886, 'r': 0.19269423005725847}\n",
      "\n",
      "Random_process nb0\n",
      "Preprocess done\n",
      "\n",
      "Time : 10.45s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.14052383114838574, 'p': 0.127256143464716, 'r': 0.1667468797675447}\n",
      "\n",
      "####overlap-5-2####\n",
      "Loading done\n"
     ]
    }
   ],
   "source": [
    "len_summary = 20\n",
    "\n",
    "def run_summaries(option, len_summary, rouge_score, corpus = \"dmoz\", language = \"french\", sampling = 1):\n",
    "    \"\"\"\n",
    "    :param option:       List of summarizer process to be tested.\n",
    "    :param len_summary:  Number of words forming the summary.\n",
    "    :param rouge_score:  Empty dictionary in wich ROUGE L median scores will be stored.\n",
    "    :param corpus:       String to indentify the corpus to use. (Not sure of that one)\n",
    "    :param language:     String to indicate the language.\n",
    "                         If set, allows to use the stop words of the given language.\n",
    "                         Default value is \"french\".\n",
    "    :param sampling:     Threshold value for document selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of hypotetic / predicted summaries generated with a summarizer\n",
    "    pred_sum = []\n",
    "    k = 0\n",
    "    \n",
    "    # Loads stop words for a given language if set.\n",
    "    stops = set() if language is None else set(stopwords.words(language))\n",
    "    \n",
    "    # Choose the 'cut' for the corpuses.\n",
    "    # Choix du d√©coupage des corpus\n",
    "    for gen in [('brutal',i*5) for i in range(1,4)]+[('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] :\n",
    "        print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "        \n",
    "        # Corpus generation.\n",
    "        if corpus == \"dmoz\" :\n",
    "            docs, gold_tokenized_summaries, overall = generate_corpus(*gen, sampling = sampling)\n",
    "        else :\n",
    "            docs, gold_tokenized_summaries, overall  = generate_corpus_duc(*gen, sampling = sampling)\n",
    "        \n",
    "        \n",
    "        for summarizer in option :\n",
    "            \n",
    "            # Creation of hypotetic summaries with the summarizer being tested.\n",
    "            print(\"\\n\"+summarizer.__name__)\n",
    "            s = time.time()\n",
    "            pred_sum.append(doc_summarizer(docs, summarizer, len_summary, 0 if len(gen) < 3 else gen[2]))\n",
    "            e = time.time()\n",
    "            print(\"\\nTime :\", \"{:.2f}s\".format(e-s))\n",
    "            print(\"Summary done\")\n",
    "            \n",
    "            # Cr√©ation des phrases\n",
    "            all_hypothesis = []\n",
    "            all_references = []\n",
    "            # For each word in our documents and in \n",
    "            for ref in set(overall.keys()).intersection(docs.keys()) :\n",
    "                if len(gen)>2 :\n",
    "\n",
    "                    pred_sum_words = set()\n",
    "                    sen = \"\"\n",
    "                    for sen_id in pred_sum[k][ref]:\n",
    "                        segment = docs[ref][1][sen_id].split()\n",
    "                        for i in range(sen_id*gen[2], sen_id*gen[2] + gen[1]) :\n",
    "                            if i in pred_sum_words :\n",
    "                                pass\n",
    "                            else :\n",
    "                                pred_sum_words.add(i)\n",
    "                                sen += \" \" + segment[i-sen_id*gen[2]]\n",
    "                                if len(pred_sum_words) == len_summary :\n",
    "                                    break\n",
    "                        else :\n",
    "                            continue\n",
    "                        break\n",
    "\n",
    "                    hypothesis = sen\n",
    "\n",
    "                else :\n",
    "                    hypothesis = join_and_cut_at([docs[ref][1][sen_id] for sen_id in pred_sum[k][ref]],\n",
    "                                                          len_summary, stops)\n",
    "                for anot in gold_tokenized_summaries[ref] :\n",
    "                    all_hypothesis.append(hypothesis)\n",
    "                    all_references.append(join_and_cut_at(gold_tokenized_summaries[ref][anot][1], len_summary, stops))\n",
    "            print(\"Rouge preprocess done\")\n",
    "\n",
    "            #Calcul des scores rouges.\n",
    "            under = [x[\"rouge-1\"] for x in evaluator.get_scores(all_hypothesis, all_references)]\n",
    "            med = { k : sum(t[k] for t in under)/len(under) for k in under[0] }\n",
    "            rouge_score[\"-\".join([str(x) for x in gen])][summarizer.__name__] = {\"rouge-1\" : med}\n",
    "            print(\"Rouge done : \"+ str(med))\n",
    "            \n",
    "            k+=1\n",
    "    #Renvoies les r√©sum√©s.\n",
    "    return pred_sum\n",
    "\n",
    "resumes = run_summaries(option, len_summary, rouge_score, language=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumy\n",
    "\n",
    "Testing of the [sumy library](#https://pypi.org/project/sumy/).\n",
    "\n",
    "Why is it not done with the other summarizer processes ? God only knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sumy_summaries = []\n",
    "\n",
    "# Loads stop words for a given language if set.\n",
    "stops = set()\n",
    "\n",
    "# Choose the 'cut' for the corpuses.\n",
    "# Choix du d√©coupage des corpus\n",
    "for gen in [('brutal',i*5) for i in range(1,4)]+[('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] :\n",
    "    print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "\n",
    "docs, gold_tokenized_summaries, overall = generate_corpus(*gen, sampling = 1)\n",
    "for summarizer in summy_option :\n",
    "    print(\"\\n\"+summarizer.__name__)\n",
    "    d = {}\n",
    "    i = 0\n",
    "    l = len(docs)\n",
    "    for ref in docs.keys() :\n",
    "        i+=1\n",
    "        d[ref] = summarizer.summarize(ref, docs, True)\n",
    "        print(\"\\r{:.2%}\".format(i/l),end = \" \")\n",
    "    sumy_summaries.append(d)\n",
    "    \n",
    "    print(\"\\nSummary done\")\n",
    "    all_hypothesis = []\n",
    "    all_references = []\n",
    "    for ref in overall.keys() :\n",
    "        hypothesis = [ \"\\n\".join(sumy_summaries[-1][ref])] \n",
    "        references = [[ \"\\n\".join(gold_tokenized_summaries[ref][ano][0]) for ano in gold_tokenized_summaries[ref] ]]\n",
    "        \n",
    "        all_hypothesis += hypothesis\n",
    "        all_references += references\n",
    "    rouge_score[option.__name__] = evaluator.get_scores(all_hypothesis, all_references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "    \n",
    "#Used to turn the dictionary into a usable dataframe.\n",
    "rowsR = []\n",
    "for g in rouge_score :\n",
    "    for m in rouge_score[g] :\n",
    "        for s in rouge_score[g][m] :\n",
    "            rowsR.append({\"M√©thode\" : m, \"Score\" : g, \"Recall\" : rouge_score[g][m][s]['r']})\n",
    "replacement = ([\"brutal-5\", \"overlap-5-2\"],[\"brutal-05\", \"overlap-05-2\"])\n",
    "dataR = pd.DataFrame(rowsR, columns = [\"M√©thode\",\"Score\", \"Recall\"]).replace(*replacement)\n",
    "\n",
    "#For showing datas in a graph\n",
    "sns.lineplot(data=dataR, x=\"Score\", y=\"Recall\",hue=\"M√©thode\" )\n",
    "plt.legend(bbox_to_anchor=(0.01, 0.3), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfR = pd.DataFrame(index = sorted(set(dataR[\"M√©thode\"])),\n",
    "                   columns = sorted(set(dataR[\"Score\"]))).rename_axis(\"Rappel\", axis=\"columns\")\n",
    "for r in dataR.iterrows()  :\n",
    "    dfR.at[r[1][\"M√©thode\"], r[1][\"Score\"]] = r[1][\"Recall\"]\n",
    "from IPython.display import display\n",
    "display(dfR.sort_values(['brutal-05'],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
