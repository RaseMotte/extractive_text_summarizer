{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizers Interface\n",
    "\n",
    "\n",
    "This file instantiates all general parameters linked to model testing (and run the tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et chargement des méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%run Process_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/Generic_Summarizer.ipynb\n",
    "%run Summary_Processes/Baseline_Summay.ipynb\n",
    "%run Summary_Processes/Random_Summary.ipynb\n",
    "%run Summary_Processes/TextRank_TFIDF_Assym_Summary.ipynb\n",
    "\n",
    "%run datasets_utils/duc.ipynb\n",
    "%run datasets_utils/dmoz.ipynb\n",
    "%run datasets_utils/dmoz_html.ipynb\n",
    "\n",
    "import os\n",
    "import rouge\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pprint import pprint as pp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers Initialization\n",
    "\n",
    "Define the _option_ variable which a list of summarization processes to test.\n",
    "\n",
    "* Each summarization process is instantiaded with its options.\n",
    "* A summarization process is a model and its routines.\n",
    "* Models and their routines are defined in the [Summary_Processes folder](http://localhost:8888/tree/web-summary/Summary_Processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running summarizers\")\n",
    "import math\n",
    "#List of all options \n",
    "def const_1(x):\n",
    "    return 1\n",
    "def const_2(x):\n",
    "    return 2\n",
    "emb_fol = \"../wdl/clustering/crawl-dmoz-fr/docWords/\"\n",
    "option = ([#Baseline_process(),\n",
    "           Random_process(),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"tr\", lsanbcompfun = math.log, diag = \"none\"),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False,\n",
    "          #                                        method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          # Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4, ldanbcompfun = const_1, bias = 0.0),\n",
    "          #Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4,\n",
    "          #                   method = \"lsa\", ldanbcompfun = const_1),\n",
    "         ]\n",
    "          + [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "              for diag in [\"none\",\"before\"] for i in range(1) for method in [\"tr\", \"lsa\"] for fun in [const_1] for bias in [3]]\n",
    "         #+ [ TextRank_Jaccard_process(1, 0.25*i, weighted = False, method = m, lsanbcompfun = const_1, diag = diag, bias = 0.5*bias)\n",
    "         #     for i in range(1) for diag in (\"none\", \"before\") for m in (\"tr\", \"lsa\") for bias in [3]] \n",
    "         #+ [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "          #+ [Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "         #                    emb_fol, a = 1, b = 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "         )\n",
    "\n",
    "\n",
    "folder = \"../wdl/resume/docs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers testing process\n",
    "\n",
    "* Load stop words (optional)\n",
    "* Cut the corpus (is this tokenization ?)\n",
    "* Generate corpuses\n",
    "* Mystic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "SCORE_DIR = \"../scores\"\n",
    "SCORE_FILE = os.path.join(SCORE_DIR, \"%s_bias_tfidf_summarizers_scores.csv\" % date.today().strftime(\"%d-%m-%y\"))\n",
    "COLS_NAME = [\"summarizer_name\", \"exec_time\", \"avg_rl_f\", \"avg_rl_p\", \"avg_rl_r\", \"text_cut\", \"sampling_size\"]\n",
    "\n",
    "def save_summarizer_score(summarizer_name, time, average_score, text_cut, sampling_size, clear_prev=False):\n",
    "    \"\"\"\n",
    "    Saves the avergae rouge score of a summarizer.\n",
    "    \n",
    "    Creates a file named summarizers_scores.csv and adds the different variables for the model.\n",
    "    CSV header is defined as follow: summarizer_name,exec_time,avg_score,sampling_size\n",
    "    \n",
    "    :param summarizer_name:    Name of the summarizer model.\n",
    "    :param time:               Time the model took to create the summaries.\n",
    "    :param average_scores:     The average rouge-l score of the model.\n",
    "    :param sampling_size:      The % of document selected.\n",
    "    :param clear_prev:         Wether previous entries for this model should be deleted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates the directory for scores if it doesn't exists.\n",
    "    if not os.path.exists(SCORE_DIR):\n",
    "        os.makedirs(SCORE_DIR)\n",
    "        \n",
    "    # Prepare the row to save in the data frame\n",
    "    new_row = [summarizer_name, time, average_score[\"f\"], average_score[\"p\"], average_score[\"r\"],\n",
    "               str(text_cut), sampling_size]\n",
    "    \n",
    "    # Loads the csv file if it exists\n",
    "    if os.path.exists(SCORE_FILE):\n",
    "        df = pd.read_csv(SCORE_FILE, index_col=0)\n",
    "        df = df.append(dict(zip(COLS_NAME, new_row)), ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame(np.array([new_row]), columns = COLS_NAME)\n",
    "    df.to_csv(SCORE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_gen(corpus, language, sampling):\n",
    "    biased_vocab = None\n",
    "    if corpus == \"dmoz\":\n",
    "        docs, gold_tokenized_summaries = generate_corpus(language, sampling = sampling)\n",
    "    elif corpus == \"dmoz-html\":\n",
    "        docs, gold_tokenized_summaries, biased_vocab = generate_corpus_dmoz_html(language, sampling = sampling)\n",
    "    else:\n",
    "        docs, gold_tokenized_summaries = generate_corpus_duc(language, sampling = sampling)\n",
    "    return docs, gold_tokenized_summaries, biased_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_preprocess(docs, gold_sum_dict, pred_sum, overall, len_summary, gen, stops, k):\n",
    "    # Création des phrases\n",
    "    all_hypothesis = []\n",
    "    all_references = []\n",
    "    \n",
    "    for doc_url in set(overall.keys()).intersection(docs.keys()) :\n",
    "        if len(gen) > 2:\n",
    "            pred_sum_words = set()\n",
    "            sen = \"\"\n",
    "            for sen_id in pred_sum[k][doc_url]:\n",
    "                segment = docs[doc_url][sen_id].split()\n",
    "                for i in range(sen_id * gen[2], sen_id * gen[2] + gen[1]):\n",
    "                    if i in pred_sum_words:\n",
    "                        pass\n",
    "                    else :\n",
    "                        pred_sum_words.add(i)\n",
    "                        sen += \" \" + segment[i - sen_id * gen[2]]\n",
    "                        if len(pred_sum_words) == len_summary :\n",
    "                            break\n",
    "                else :\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            hypothesis = sen\n",
    "        else :\n",
    "            hypothesis = join_and_cut_at([docs[doc_url][sen_id] for sen_id in pred_sum[k][doc_url]],\n",
    "                                                  len_summary, stops)\n",
    "        all_hypothesis.append(hypothesis)\n",
    "        all_references.append(join_and_cut_at(gold_sum_dict[doc_url], len_summary, stops))\n",
    "    print(\"Rouge preprocess done\")\n",
    "    return all_hypothesis, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def run_summaries(summarizer_test_list, len_summary, rouge_score, corpus, language, sampling=1.0):\n",
    "    \"\"\"\n",
    "    :param summarizer_test_list:       List of summarizer process to be tested.\n",
    "    :param len_summary:  Number of words making the final summary.\n",
    "    :param rouge_score:  Empty dictionary in wich ROUGE L median scores will be stored.\n",
    "    :param corpus:       String to indentify the corpus to use.\n",
    "    :param language:     String to indicate the language.\n",
    "                         If set, allows to use the stop words of the given language.\n",
    "    :param sampling:     Threshold value for document selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of hypotetic / predicted summaries generated with a summarizer\n",
    "    pred_sum = []\n",
    "    k = 0\n",
    "    \n",
    "    raw_docs, raw_gold_sum_dict, raw_biased_vocab = corpus_gen(corpus, language, sampling = sampling)\n",
    "\n",
    "    # Corpus necessary for tfidf fit.\n",
    "    # Array of strings. Each string is a sentence token. The whole dataset is flatten, document are not separated.\n",
    "    corpus = [doc for doc in raw_docs.values()]\n",
    "    \n",
    "    # Build the vectorizer only once. \n",
    "    # Builds the idf matrix with all sentence tokens of the corpus.\n",
    "    # Also builds the vocabulary of the corpus. It's a dictionay mapping a word to its feature indice (index).\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Learn our representation space, ie, its dimension (vocabulary size) and the idf factors.\n",
    "    vectorizer.fit(corpus)\n",
    "    \n",
    "    # Build the biased vocabulary : a dictionnary mapping a word index to its weight.\n",
    "    biased_vocab = None\n",
    "    if raw_biased_vocab is not None:\n",
    "        vocab = vectorizer.vocabulary_\n",
    "        biased_vocab = build_vocab_bias(vocab, raw_biased_vocab, get_html_bias())\n",
    "\n",
    "    # Choix du découpage des corpus\n",
    "    #for gen in [('overlap', 5, 2)]:\n",
    "    #for gen in [('brutal', 5)] + [('overlap', 5, 2), ('overlap', 10, 4)]+[('nltk',)] + [('spacy',)]:\n",
    "    for gen in [('brutal',i*5) for i in range(1,4)] + [('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] + [('spacy',)]:\n",
    "        print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "        \n",
    "        # Loads stop words for a given language if set.\n",
    "        stops = set() #if language is None else set(stopwords.words(language))\n",
    "        \n",
    "        docs, gold_sum_dict = prepare_corpus(language, gen, raw_docs, raw_gold_sum_dict)\n",
    "        # Build overall\n",
    "        overall = {x : \"\" for x in set(docs.keys()).intersection(gold_sum_dict.keys())}\n",
    "        assert(len(overall) != 0)\n",
    "        assert(len(docs) != 0)\n",
    "            \n",
    "        for summarizer in summarizer_test_list :\n",
    "            # Set the summarizer of the model to the unique build\n",
    "            summarizer.vectorizer = vectorizer\n",
    "            \n",
    "            # Creation of hypotetic summaries with the    being tested.\n",
    "            print(\"\\n\"+summarizer.__name__)\n",
    "            s = time.time()\n",
    "            pred_sum.append(doc_summarizer(docs, summarizer, len_summary, 0 if len(gen) < 3 else gen[2], biased_vocab))\n",
    "            e = time.time()\n",
    "            print(\"\\nTime :\", \"{:.2f}s\".format(e-s))\n",
    "            print(\"Summary done\")\n",
    "            \n",
    "            assert(len(docs) == len(pred_sum[k]))\n",
    "            \n",
    "            # If overlap tokenization method was used, rebuild the sentences as initially found in the document.\n",
    "            all_hypothesis, all_references = rouge_preprocess(docs, gold_sum_dict, pred_sum, overall, len_summary,\n",
    "                                                              gen, stops, k)\n",
    "            assert(len(all_hypothesis) == len(all_references))\n",
    "\n",
    "            #Calcul des scores rouges.\n",
    "            under = [x[\"rouge-1\"] for x in evaluator.get_scores(all_hypothesis, all_references)]\n",
    "            med = { k : sum(t[k] for t in under)/len(under) for k in under[0] }\n",
    "            rouge_score[\"-\".join([str(x) for x in gen])][summarizer.__name__] = {\"rouge-1\" : med}\n",
    "            print(\"Rouge done : \"+ str(med))\n",
    "            save_summarizer_score(summarizer.__name__, \"{:.2f}s\".format(e-s), med, \"-\".join([str(x) for x in gen]), sampling)\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "    # Return predicted summaries\n",
    "    return docs, gold_sum_dict, pred_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defines an empty dictionary in wich median ROUGE L scores will be stored.\n",
    "evaluator = rouge.Rouge(metrics=['rouge-1'],)\n",
    "rouge_score = defaultdict(dict)\n",
    "\n",
    "len_summary = 20\n",
    "\n",
    "docs, gold, summaries = run_summaries(option,\n",
    "                                      len_summary,\n",
    "                                      rouge_score,\n",
    "                                      corpus=\"dmoz-html\",\n",
    "                                      language=\"french\",\n",
    "                                      sampling=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def html_gen(docs, summaries):\n",
    "    for doc_url in set(docs.keys()).intersection(set(summaries[0].keys())) :\n",
    "        \n",
    "        #Head string with name of the file\n",
    "        s = '<h2>'+ doc_url +'</h2><p style=\"text-align:justify\">'\n",
    "        \n",
    "        #Color Modifiers\n",
    "        m1, m2, m3 = 0,0,0\n",
    "        \n",
    "        for i in range(len(docs[doc_url])) :\n",
    "            k = len(summaries)\n",
    "            \n",
    "            #Color assignement\n",
    "            m1 = 160 * (i in summaries[0][doc_url])\n",
    "            if k-1 :\n",
    "                m2 = 160 * (i in summaries[1][doc_url])\n",
    "                if k-2 :\n",
    "                    m3 = 160 * (i in summaries[2][doc_url])\n",
    "            \n",
    "            #Generating the sentence colored\n",
    "            s += ('<span style=\"color:rgb('+str(m1)+','+str(m2)+','+str(m3)+')'\n",
    "                +';background-color:rgb('+str(255)+',255,'+str(255)+')\">'\n",
    "                + docs[doc_url][i]\n",
    "                +\" </span>\")\n",
    "        \n",
    "        #Display of the sentence\n",
    "        display(HTML(s+\"</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_docs = dict(list(docs.items())[:10])\n",
    "html_gen(display_docs, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(gold.values())[0])\n",
    "\n",
    "#html_gen(display_docs, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "    \n",
    "#Used to turn the dictionary into a usable dataframe.\n",
    "rowsR = []\n",
    "for g in rouge_score :\n",
    "    for m in rouge_score[g] :\n",
    "        for s in rouge_score[g][m] :\n",
    "            rowsR.append({\"Méthode\" : m, \"Score\" : g, \"Recall\" : rouge_score[g][m][s]['r']})\n",
    "replacement = ([\"brutal-5\", \"overlap-5-2\"],[\"brutal-05\", \"overlap-05-2\"])\n",
    "dataR = pd.DataFrame(rowsR, columns = [\"Méthode\",\"Score\", \"Recall\"]).replace(*replacement)\n",
    "\n",
    "#For showing datas in a graph\n",
    "sns.lineplot(data=dataR, x=\"Score\", y=\"Recall\",hue=\"Méthode\" )\n",
    "plt.legend(bbox_to_anchor=(0.01, 0.3), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfR = pd.DataFrame(index = sorted(set(dataR[\"Méthode\"])),\n",
    "                   columns = sorted(set(dataR[\"Score\"]))).rename_axis(\"Rappel\", axis=\"columns\")\n",
    "for r in dataR.iterrows()  :\n",
    "    dfR.at[r[1][\"Méthode\"], r[1][\"Score\"]] = r[1][\"Recall\"]\n",
    "from IPython.display import display\n",
    "display(dfR.sort_values(['brutal-05'],ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
