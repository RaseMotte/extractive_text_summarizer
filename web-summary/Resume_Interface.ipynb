{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizers Interface\n",
    "\n",
    "\n",
    "This file instantiates all general parameters linked to model testing (and run the tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et chargement des m√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/pfee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%run Process_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/Generic_Summarizer.ipynb\n",
    "\n",
    "%run Summary_Processes/Baseline_Summay.ipynb\n",
    "%run Summary_Processes/TextRank_Jaccard_Summary.ipynb\n",
    "%run Summary_Processes/Random_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/Embeddings_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TFIDF_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TextRank_TFIDF_Assym_Summary.ipynb\n",
    "\"\"\"\n",
    "%run Summary_Processes/Cluster_Summary.ipynb\n",
    "%run Summary_Processes/Cluster_First_Summary.ipynb\n",
    "\"\"\"\n",
    "%run Summary_Processes/Sumy_Summary.ipynb\n",
    "\n",
    "%run datasets_utils/duc.ipynb\n",
    "%run datasets_utils/dmoz.ipynb\n",
    "%run datasets_utils/dmoz_html.ipynb\n",
    "\n",
    "import os\n",
    "import rouge\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pprint import pprint as pp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers Initialization\n",
    "\n",
    "Define the _option_ variable which a list of summarization processes to test.\n",
    "\n",
    "* Each summarization process is instantiaded with its options.\n",
    "* A summarization process is a model and its routines.\n",
    "* Models and their routines are defined in the [Summary_Processes folder](http://localhost:8888/tree/web-summary/Summary_Processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running summarizers\n"
     ]
    }
   ],
   "source": [
    "print(\"Running summarizers\")\n",
    "import math\n",
    "#List of all options \n",
    "def const_1(x):\n",
    "    return 1\n",
    "def const_2(x):\n",
    "    return 2\n",
    "emb_fol = \"../wdl/clustering/crawl-dmoz-fr/docWords/\"\n",
    "option = ([#Baseline_process(),\n",
    "           Random_process(),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"tr\", lsanbcompfun = math.log, diag = \"none\"),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False,\n",
    "          #                                        method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          # Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4, ldanbcompfun = const_1, bias = 0.0),\n",
    "          #Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4,\n",
    "          #                   method = \"lsa\", ldanbcompfun = const_1),\n",
    "         ]\n",
    "          + [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "              for diag in [\"none\",\"before\"] for i in range(1) for method in [\"tr\", \"lsa\"] for fun in [const_1] for bias in [3]]\n",
    "         #+ [ TextRank_Jaccard_process(1, 0.25*i, weighted = False, method = m, lsanbcompfun = const_1, diag = diag, bias = 0.5*bias)\n",
    "         #     for i in range(1) for diag in (\"none\", \"before\") for m in (\"tr\", \"lsa\") for bias in [3]] \n",
    "         #+ [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "          #+ [Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "         #                    emb_fol, a = 1, b = 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "         )\n",
    "\n",
    "\n",
    "#List of options for sumy\n",
    "folder = \"../wdl/resume/docs/\"\n",
    "#summy_option = [ Sumy_process(mode, folder, \"french\", nb_sent = 2) for mode in [\"text_rank\"]]\n",
    "#, \"sum_basic\", \"kl\", \"luhn\", \"lsa\", \"lex_rank\"]]\n",
    "#summy_option = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an empty dictionary in wich median ROUGE L scores will be stored.\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-1'],)\n",
    "rouge_score = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers testing process\n",
    "\n",
    "* Load stop words (optional)\n",
    "* Cut the corpus (is this tokenization ?)\n",
    "* Generate corpuses\n",
    "* Mystic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "SCORE_DIR = \"../scores\"\n",
    "SCORE_FILE = os.path.join(SCORE_DIR, \"%s_bias_tfidf_summarizers_scores.csv\" % date.today().strftime(\"%d-%m-%y\"))\n",
    "COLS_NAME = [\"summarizer_name\", \"exec_time\", \"avg_rl_f\", \"avg_rl_p\", \"avg_rl_r\", \"text_cut\", \"sampling_size\"]\n",
    "\n",
    "def save_summarizer_score(summarizer_name, time, average_score, text_cut, sampling_size, clear_prev=False):\n",
    "    \"\"\"\n",
    "    Saves the avergae rouge score of a summarizer.\n",
    "    \n",
    "    Creates a file named summarizers_scores.csv and adds the different variables for the model.\n",
    "    CSV header is defined as follow: summarizer_name,exec_time,avg_score,sampling_size\n",
    "    \n",
    "    :param summarizer_name:    Name of the summarizer model.\n",
    "    :param time:               Time the model took to create the summaries.\n",
    "    :param average_scores:     The average rouge-l score of the model.\n",
    "    :param sampling_size:      The % of document selected.\n",
    "    :param clear_prev:         Wether previous entries for this model should be deleted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates the directory for scores if it doesn't exists.\n",
    "    if not os.path.exists(SCORE_DIR):\n",
    "        os.makedirs(SCORE_DIR)\n",
    "        \n",
    "    # Prepare the row to save in the data frame\n",
    "    new_row = [summarizer_name, time, average_score[\"f\"], average_score[\"p\"], average_score[\"r\"],\n",
    "               str(text_cut), sampling_size]\n",
    "    \n",
    "    # Loads the csv file if it exists\n",
    "    if os.path.exists(SCORE_FILE):\n",
    "        df = pd.read_csv(SCORE_FILE, index_col=0)\n",
    "        df = df.append(dict(zip(COLS_NAME, new_row)), ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame(np.array([new_row]), columns = COLS_NAME)\n",
    "    df.to_csv(SCORE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:[DMOZ HTML][GOLD LOAD] Total of gold summaries loaded 26107\n",
      "DEBUG:root:[DMOZ HTML][GOLD LOAD] Total of non empty gold summaries loaded 18272\n",
      "DEBUG:root:[DMOZ HTML][PREP] Starting deserialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####brutal-5####\n",
      "Begin dmoz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:[DMOZ HTML][PREP] Done\n",
      "DEBUG:root:[DMOZ HTML][PREP] Preprocessing pipeline\n",
      "DEBUG:root:[DMOZ HTML][PREP] Biased vocab\n",
      "DEBUG:root:[DMOZ HTML][PREP] docs\n",
      "DEBUG:root:[DMOZ HTML][PREP] docs\n",
      "DEBUG:root:[DMOZ HTML][PREP] Done\n",
      "DEBUG:root:[DMOZ HTML][OverAll] Starting\n",
      "DEBUG:root:[DMOZ HTML][OverAll] Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End dmoz\n",
      "\n",
      "Random_process nb0\n",
      "Preprocess done\n",
      "\n",
      "Time : 1.39s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.20918584257683484, 'p': 0.20161587107816492, 'r': 0.2298300447485679}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:[Preprocessors][Bias] Building vocab bias for each document\n",
      "DEBUG:root:[Preprocessors][Bias] Done, Time :37.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess done\n",
      "\n",
      "Time : 3520.41s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.3185627669848609, 'p': 0.3113594327719861, 'r': 0.3430792772575112}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:[Preprocessors][Bias] Building vocab bias for each document\n",
      "DEBUG:root:[Preprocessors][Bias] Done, Time :37.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess done\n"
     ]
    }
   ],
   "source": [
    "len_summary = 20\n",
    "\n",
    "def run_summaries(option, len_summary, rouge_score, corpus, language, sampling=1.0):\n",
    "    \"\"\"\n",
    "    :param option:       List of summarizer process to be tested.\n",
    "    :param len_summary:  Number of words forming the summary.\n",
    "    :param rouge_score:  Empty dictionary in wich ROUGE L median scores will be stored.\n",
    "    :param corpus:       String to indentify the corpus to use.\n",
    "    :param language:     String to indicate the language.\n",
    "                         If set, allows to use the stop words of the given language.\n",
    "                         Default value is \"french\".\n",
    "    :param sampling:     Threshold value for document selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of hypotetic / predicted summaries generated with a summarizer\n",
    "    pred_sum = []\n",
    "    k = 0\n",
    "    \n",
    "    # Loads stop words for a given language if set.\n",
    "    stops = set() #if language is None else set(stopwords.words(language))\n",
    "    \n",
    "    # Choose the 'cut' for the corpuses.\n",
    "    # Choix du d√©coupage des corpus\n",
    "    \n",
    "    #for gen in [('overlap', 5, 2)]:\n",
    "    for gen in [('brutal',i*5) for i in range(1,4)] + [('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] + [('spacy',)]:\n",
    "        print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "        \n",
    "        # Corpus generation.\n",
    "        doc_bias = None\n",
    "        if corpus == \"dmoz\" :\n",
    "            docs, gold_tokenized_summaries, overall = generate_corpus(language, *gen, sampling = sampling)\n",
    "        elif corpus == \"dmoz-html\":\n",
    "            print(\"Begin dmoz\")\n",
    "            docs, gold_tokenized_summaries, overall, docs_bias = generate_corpus_dmoz_html(language, *gen, sampling = sampling)\n",
    "            print(\"End dmoz\")\n",
    "        else :\n",
    "            docs, gold_tokenized_summaries, overall  = generate_corpus_duc(language, *gen, sampling = sampling)\n",
    "        \n",
    "        for summarizer in option :\n",
    "            \n",
    "            # Creation of hypotetic summaries with the summarizer being tested.\n",
    "            print(\"\\n\"+summarizer.__name__)\n",
    "            s = time.time()\n",
    "            pred_sum.append(doc_summarizer(docs, summarizer, len_summary, 0 if len(gen) < 3 else gen[2], docs_bias))\n",
    "            e = time.time()\n",
    "            print(\"\\nTime :\", \"{:.2f}s\".format(e-s))\n",
    "            print(\"Summary done\")\n",
    "            \n",
    "            assert(len(docs) == len(pred_sum[k]))\n",
    "            \n",
    "            # Cr√©ation des phrases\n",
    "            all_hypothesis = []\n",
    "            all_references = []\n",
    "            \n",
    "            # For each document (doc_id) in our documents and summaries sets\n",
    "            for ref in set(overall.keys()).intersection(docs.keys()) :\n",
    "                if len(gen) > 2:\n",
    "                    pred_sum_words = set()\n",
    "                    sen = \"\"\n",
    "                    for sen_id in pred_sum[k][ref]:\n",
    "                        segment = docs[ref][sen_id].split()\n",
    "                        for i in range(sen_id * gen[2], sen_id * gen[2] + gen[1]):\n",
    "                            if i in pred_sum_words:\n",
    "                                pass\n",
    "                            else :\n",
    "                                pred_sum_words.add(i)\n",
    "                                sen += \" \" + segment[i - sen_id * gen[2]]\n",
    "                                if len(pred_sum_words) == len_summary :\n",
    "                                    break\n",
    "                        else :\n",
    "                            continue\n",
    "                        break\n",
    "\n",
    "                    hypothesis = sen\n",
    "                else :\n",
    "                    hypothesis = join_and_cut_at([docs[ref][sen_id] for sen_id in pred_sum[k][ref]],\n",
    "                                                          len_summary, stops)\n",
    "                    #print(\"docs[ref][sen_id] =\", [docs[ref][sen_id] for sen_id in pred_sum[k][ref]])\n",
    "                    #print(\"hypothesis\", hypothesis)\n",
    "                    #hypothesis = join_and_cut_at([docs[ref][1][sen_id] for sen_id in pred_sum[k][ref]],\n",
    "                    #                                     len_summary, stops)\n",
    "                #for anot in gold_tokenized_summaries[ref]:\n",
    "                #    all_hypothesis.append(hypothesis)\n",
    "                #    all_references.append(join_and_cut_at(gold_tokenized_summaries[ref][anot][1], len_summary, stops))\n",
    "                all_hypothesis.append(hypothesis)\n",
    "                all_references.append(join_and_cut_at(gold_tokenized_summaries[ref], len_summary, stops))\n",
    "            print(\"Rouge preprocess done\")\n",
    "\n",
    "            #Calcul des scores rouges.\n",
    "            under = [x[\"rouge-1\"] for x in evaluator.get_scores(all_hypothesis, all_references)]\n",
    "            med = { k : sum(t[k] for t in under)/len(under) for k in under[0] }\n",
    "            rouge_score[\"-\".join([str(x) for x in gen])][summarizer.__name__] = {\"rouge-1\" : med}\n",
    "            print(\"Rouge done : \"+ str(med))\n",
    "            save_summarizer_score(summarizer.__name__, \"{:.2f}s\".format(e-s), med, \"-\".join([str(x) for x in gen]), sampling)\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "    # Return predicted summaries\n",
    "    return docs, gold_tokenized_summaries, pred_sum\n",
    "\n",
    "docs, gold, summaries = run_summaries(option, len_summary, rouge_score, corpus=\"dmoz-html\", language=\"french\", sampling=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sumy tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sumy_summaries = []\n",
    "\n",
    "# Loads stop words for a given language if set.\n",
    "stops = set()\n",
    "\n",
    "# Choose the 'cut' for the corpuses.\n",
    "# Choix du d√©coupage des corpus\n",
    "for gen in [('brutal',i*5) for i in range(1,4)]+[('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] :\n",
    "    print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "\n",
    "docs, gold_tokenized_summaries, overall = generate_corpus(*gen, sampling = 1)\n",
    "for summarizer in summy_option :\n",
    "    print(\"\\n\"+summarizer.__name__)\n",
    "    d = {}\n",
    "    i = 0\n",
    "    l = len(docs)\n",
    "    for ref in docs.keys() :\n",
    "        i+=1\n",
    "        d[ref] = summarizer.summarize(ref, docs, True)\n",
    "        print(\"\\r{:.2%}\".format(i/l),end = \" \")\n",
    "    sumy_summaries.append(d)\n",
    "    \n",
    "    print(\"\\nSummary done\")\n",
    "    all_hypothesis = []\n",
    "    all_references = []\n",
    "    for ref in overall.keys() :\n",
    "        hypothesis = [ \"\\n\".join(sumy_summaries[-1][ref])] \n",
    "        references = [[ \"\\n\".join(gold_tokenized_summaries[ref][ano][0]) for ano in gold_tokenized_summaries[ref] ]]\n",
    "        all_hypothesis += hypothesis\n",
    "        all_references += references\n",
    "    rouge_score[option.__name__] = evaluator.get_scores(all_hypothesis, all_references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_gen(docs, summaries):\n",
    "    for doc in set(docs.keys()).intersection(set(summaries[0].keys())) :\n",
    "        \n",
    "        #Head string with name of the file\n",
    "        s = '<h2>'+doc+'</h2><p style=\"text-align:justify\">'\n",
    "        \n",
    "        #Color Modifiers\n",
    "        m1, m2, m3 = 0,0,0\n",
    "        \n",
    "        for i in range(len(docs[doc][0])) :\n",
    "            k = len(summaries)\n",
    "            \n",
    "            #Color assignement\n",
    "            m1 = 160 * (i in summaries[0][doc])\n",
    "            if k-1 :\n",
    "                m2 = 160 * (i in summaries[1][doc])\n",
    "                if k-2 :\n",
    "                    m3 = 160 * (i in summaries[2][doc])\n",
    "            \n",
    "            #Generating the sentence colored\n",
    "            s += ('<span style=\"color:rgb('+str(m1)+','+str(m2)+','+str(m3)+')'\n",
    "                +';background-color:rgb('+str(255)+',255,'+str(255)+')\">'\n",
    "                + docs[doc][0][i]\n",
    "                +\" </span>\")\n",
    "        \n",
    "        #Display of the sentence\n",
    "        display(HTML(s+\"</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_gen(docs[:10], summaries[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_gen(docs[:10], gold[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "    \n",
    "#Used to turn the dictionary into a usable dataframe.\n",
    "rowsR = []\n",
    "for g in rouge_score :\n",
    "    for m in rouge_score[g] :\n",
    "        for s in rouge_score[g][m] :\n",
    "            rowsR.append({\"M√©thode\" : m, \"Score\" : g, \"Recall\" : rouge_score[g][m][s]['r']})\n",
    "replacement = ([\"brutal-5\", \"overlap-5-2\"],[\"brutal-05\", \"overlap-05-2\"])\n",
    "dataR = pd.DataFrame(rowsR, columns = [\"M√©thode\",\"Score\", \"Recall\"]).replace(*replacement)\n",
    "\n",
    "#For showing datas in a graph\n",
    "sns.lineplot(data=dataR, x=\"Score\", y=\"Recall\",hue=\"M√©thode\" )\n",
    "plt.legend(bbox_to_anchor=(0.01, 0.3), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfR = pd.DataFrame(index = sorted(set(dataR[\"M√©thode\"])),\n",
    "                   columns = sorted(set(dataR[\"Score\"]))).rename_axis(\"Rappel\", axis=\"columns\")\n",
    "for r in dataR.iterrows()  :\n",
    "    dfR.at[r[1][\"M√©thode\"], r[1][\"Score\"]] = r[1][\"Recall\"]\n",
    "from IPython.display import display\n",
    "display(dfR.sort_values(['brutal-05'],ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
