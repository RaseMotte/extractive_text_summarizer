{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizers Interface\n",
    "\n",
    "\n",
    "This file instantiates all general parameters linked to model testing (and run the tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et chargement des m√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/utilisateur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%run Process_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/Generic_Summarizer.ipynb\n",
    "\n",
    "%run Summary_Processes/Baseline_Summay.ipynb\n",
    "%run Summary_Processes/Random_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TextRank_Jaccard_Summary.ipynb\n",
    "%run Summary_Processes/Embeddings_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TFIDF_Summary.ipynb\n",
    "\n",
    "%run Summary_Processes/TextRank_TFIDF_Assym_Summary.ipynb\n",
    "\"\"\"\n",
    "%run Summary_Processes/Cluster_Summary.ipynb\n",
    "%run Summary_Processes/Cluster_First_Summary.ipynb\n",
    "\"\"\"\n",
    "%run Summary_Processes/Sumy_Summary.ipynb\n",
    "\n",
    "%run dmoz_html_utils.ipynb\n",
    "\n",
    "import os\n",
    "import rouge\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pprint import pprint as pp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_gen(docs, resumes) :\n",
    "    for doc in set(docs.keys()).intersection(set(resumes[0].keys())) :\n",
    "        \n",
    "        #Head string with name of the file\n",
    "        s = '<h2>'+doc+'</h2><p style=\"text-align:justify\">'\n",
    "        \n",
    "        #Color Modifiers\n",
    "        m1, m2, m3 = 0,0,0\n",
    "        \n",
    "        for i in range(len(docs[doc][0])) :\n",
    "            k = len(resumes)\n",
    "            \n",
    "            #Color assignement\n",
    "            m1 = 160 * (i in resumes[0][doc])\n",
    "            if k-1 :\n",
    "                m2 = 160 * (i in resumes[1][doc])\n",
    "                if k-2 :\n",
    "                    m3 = 160 * (i in resumes[2][doc])\n",
    "            \n",
    "            #Generating the sentence colored\n",
    "            s += ('<span style=\"color:rgb('+str(m1)+','+str(m2)+','+str(m3)+')'\n",
    "                +';background-color:rgb('+str(255)+',255,'+str(255)+')\">'\n",
    "                + docs[doc][0][i]\n",
    "                +\" </span>\")\n",
    "        \n",
    "        #Display of the sentence\n",
    "        display(HTML(s+\"</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_duc(method = 'nltk', len_sen = 10, over = 4, sampling = 1 ) :\n",
    "    \"\"\"\n",
    "    Generate a corpus from the DUC dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String. Reference a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     int. Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ???\n",
    "                        Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    float. Threshold. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset id + docid to a parsed and tokenized document.\n",
    "                      'd061j/AP880911-0016' -> list(list of sentences unprocessed, list of cleaned sentences)\n",
    "                golden_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset id + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"\n",
    "    # Used for generating the corpus from duc\n",
    "    ## Preprocess des documents\n",
    "    doc_folder = \"../data/DUC/docs/\"\n",
    "    doc_corpus = {}\n",
    "    walker = os.walk(doc_folder)\n",
    "    _ = next(walker)\n",
    "\n",
    "    #Read all documents in subfolders  of the original folder\n",
    "    for docset, _, docnames in walker :\n",
    "        for docname in docnames :\n",
    "            docpath = os.path.join(docset, docname)\n",
    "            parsed_doc = parse_file(docpath, TextHandler())\n",
    "            if parsed_doc :\n",
    "                if random.random() < sampling:\n",
    "                    doc_key = os.path.join(docset.split(\"/\")[-1], docname)\n",
    "                    doc_corpus[doc_key] = parsed_doc\n",
    "    print(\"Loading done\")\n",
    "    \n",
    "    #Sentence Tokenization of the corpus\n",
    "    if method == 'nltk' :\n",
    "        tokenized_docs = tokenizer_cleaner(doc_corpus)\n",
    "    elif method == 'brutal' :\n",
    "        tokenized_docs = brutal_tokenizer(doc_corpus, len_sen)\n",
    "    elif method == 'overlap' :\n",
    "        tokenized_docs = overlap_tokenizer(doc_corpus, len_sen, over)\n",
    "    else :\n",
    "        print(\"Method not accepted: %s\" % method)\n",
    "    \n",
    "    #Cleaning part\n",
    "    tokenized_docs = {k : tokenized_docs[k] for k in tokenized_docs if len(tokenized_docs[k][1])> 3}\n",
    "    \n",
    "    #Generating summaries\n",
    "    summary_folder = \"../data/DUC/summaries/\"\n",
    "    summary_corpus = get_summaries(summary_folder)\n",
    "    overall = summary_doc_linker(summary_corpus, tokenized_docs)\n",
    "    gold_tokenized_summaries = {x : tokenizer_cleaner(summary_corpus[x]) for x in summary_corpus}\n",
    "    return tokenized_docs, gold_tokenized_summaries, overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus dmoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation corpus from the file\n",
    "def generate_corpus(method = 'nltk', len_sen = 10, over = 4, sampling = 1 ) :\n",
    "    \"\"\"\n",
    "    Generate a corpus from the dmzo dataset with documents and summaries.\n",
    "    \n",
    "    :param method:      String referencing a tokenize method.\n",
    "                        'nltk'    ->\n",
    "                        'brutal'  ->\n",
    "                        'overlap' ->\n",
    "                        Default is nltk.\n",
    "                        \n",
    "    :param len_sen:     Number of words in a sentence.\n",
    "                        Used by the 'brutal' and 'overlap' tokenizer.\n",
    "                        \n",
    "    :param over:        ??? Someting used by the 'overlap' tokenizer.\n",
    "    \n",
    "    :param sampling:    Threshold. Float. Must be between 0.0 and 1.0\n",
    "                        For each document in the data set, a random number\n",
    "                        is drawn (between 0 and 1). If smaller than the\n",
    "                        threshold, the document is kept in the final corpus.\n",
    "                        Else, it's discarded.\n",
    "                        \n",
    "    :return:    docs: Dictionary mapping string to a string.\n",
    "                      Maps a docset + docid to a parsed and tokenized document.\n",
    "                golden_summaries: Dictionary mapping a string to a dictionary.\n",
    "                      Maps a docset + docid to multiple parsed and tokenized summaries.\n",
    "                overall: Dictionary\n",
    "    \"\"\"    \n",
    "    corpus = {}\n",
    "    with open(\"../data/dmoz/dmoz-fr-content.tsv\") as file :\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line :\n",
    "            key, data = line[:-1].split(\"\\t\")\n",
    "            if random.random() < sampling:\n",
    "                corpus[key] = data\n",
    "            line = file.readline()\n",
    "            i+=1\n",
    "\n",
    "    print(\"Loading done\")\n",
    "\n",
    "    #Sentence tokenizing part\n",
    "    if method == 'nltk' :\n",
    "        docs = tokenizer_cleaner(corpus)\n",
    "    elif method == 'brutal' :\n",
    "        docs = brutal_tokenizer(corpus, len_sen)\n",
    "    elif method == 'overlap' :\n",
    "        docs = overlap_tokenizer(corpus, len_sen, over)\n",
    "    else :\n",
    "        print(\"Method not accepted\")\n",
    "\n",
    "    #Cleaning part\n",
    "    docs = {k : docs[k] for k in docs if len(docs[k][1])> 3}\n",
    "\n",
    "    #Summaries generator\n",
    "    summary_corpus = {}\n",
    "    #stops = set(stopwords.words('french'))\n",
    "    stops = set()\n",
    "    with open(\"../data/dmoz/dmoz-fr-description\") as file :\n",
    "        line = file.readline()\n",
    "        i = 0\n",
    "        while line :\n",
    "            key, data = line[:-1].split(\"\\t\")\n",
    "            datac = \" \".join(set(data.split()) - stops)\n",
    "            summary_corpus[key] = { \"m\" : datac }\n",
    "            line = file.readline()\n",
    "            i+=1\n",
    "    \n",
    "    gold_tokenized_summaries = {x : tokenizer_cleaner(summary_corpus[x]) for x in summary_corpus }\n",
    "    gold_tokenized_summaries = {x : gold_tokenized_summaries[x] for x in gold_tokenized_summaries\n",
    "                               if all(len(gold_tokenized_summaries[x][a][1])>0 for a in gold_tokenized_summaries[x])\n",
    "                               \n",
    "                               }\n",
    "\n",
    "    #Linking summaries and corpus\n",
    "    overall = {x : \"\" for x in set(docs.keys()).intersection(gold_tokenized_summaries.keys())}\n",
    "    \n",
    "    return docs, gold_tokenized_summaries, overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers Initialization\n",
    "\n",
    "Define the _option_ variable which a list of summarization processes to test.\n",
    "\n",
    "* Each summarization process is instantiaded with its options.\n",
    "* A summarization process is a model and its routines.\n",
    "* Models and their routines are defined in the [Summary_Processes folder](http://localhost:8888/tree/web-summary/Summary_Processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running summarizers\n"
     ]
    }
   ],
   "source": [
    "print(\"Running summarizers\")\n",
    "import math\n",
    "#List of all options \n",
    "def const_1(x):\n",
    "    return 1\n",
    "def const_2(x):\n",
    "    return 2\n",
    "emb_fol = \"../wdl/clustering/crawl-dmoz-fr/docWords/\"\n",
    "option = ([#Baseline_process(),\n",
    "          #Random_process(),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"tr\", lsanbcompfun = math.log, diag = \"none\"),\n",
    "          #TextRank_Jaccard_process(1, 0, weighted = False, method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False),\n",
    "          #TextRank_TFIDF_Summarizer_Assym_process(1, 0, weighted = False,\n",
    "          #                                        method = \"lsa\", lsanbcompfun = math.log, diag = \"before\"),\n",
    "          # Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4, ldanbcompfun = const_1, bias = 0.0),\n",
    "          #Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "          #                   emb_fol, weighted = False, exponentiation = 4,\n",
    "          #                   method = \"lsa\", ldanbcompfun = const_1),\n",
    "         ]\n",
    "         + [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = get_hmlt_biais())\n",
    "              for diag in [\"none\",\"before\"] for i in range(1) for method in [\"tr\", \"lsa\"] for fun in [const_1] ]\n",
    "         #+ [ TextRank_Jaccard_process(1, 0.25*i, weighted = False, method = m, lsanbcompfun = const_1, diag = diag, bias = 0.5*bias)\n",
    "         #     for i in range(1) for diag in (\"none\", \"before\") for m in (\"tr\", \"lsa\") for bias in [3]] \n",
    "         #+ [TextRank_TFIDF_Summarizer_Assym_process(1, 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "          #+ [Embeddings_process(\"wordsembeddings.npy\", \"wordslist.txt\", \"words.df.csv\" , \"docs-count.txt\", \n",
    "         #                    emb_fol, a = 1, b = 0.25*i, weighted = False, method = method, lsanbcompfun = fun, diag = diag, bias = 0.5*bias)\n",
    "         #     for diag in [\"none\",\"before\"] for i in range(1) for method in [\"lsa\",\"tr\"] for bias in [3] for fun in [const_1] ]\n",
    "         )\n",
    "\n",
    "\n",
    "#List of options for sumy\n",
    "folder = \"../wdl/resume/docs/\"\n",
    "summy_option = [ Sumy_process(mode, folder, \"french\", nb_sent = 2) for mode in [\"text_rank\"]]\n",
    "#, \"sum_basic\", \"kl\", \"luhn\", \"lsa\", \"lex_rank\"]]\n",
    "#summy_option = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an empty dictionary in wich median ROUGE L scores will be stored.\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-1'],)\n",
    "rouge_score = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizers testing process\n",
    "\n",
    "* Load stop words (optional)\n",
    "* Cut the corpus (is this tokenization ?)\n",
    "* Generate corpuses\n",
    "* Mystic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SCORE_DIR = \"../scores\"\n",
    "SCORE_FILE = os.path.join(SCORE_DIR, \"bias_tfidf_summarizers_scores.csv\")\n",
    "COLS_NAME = [\"summarizer_name\", \"exec_time\", \"avg_rl_f\", \"avg_rl_p\", \"avg_rl_r\", \"text_cut\", \"sampling_size\"]\n",
    "\n",
    "def save_summarizer_score(summarizer_name, time, average_score, text_cut, sampling_size, clear_prev=False):\n",
    "    \"\"\"\n",
    "    Saves the avergae rouge score of a summarizer.\n",
    "    \n",
    "    Creates a file named summarizers_scores.csv and adds the different variables for the model.\n",
    "    CSV header is defined as follow: summarizer_name,exec_time,avg_score,sampling_size\n",
    "    \n",
    "    :param summarizer_name:    Name of the summarizer model.\n",
    "    :param time:               Time the model took to create the summaries.\n",
    "    :param average_scores:     The average rouge-l score of the model.\n",
    "    :param sampling_size:      The % of document selected.\n",
    "    :param clear_prev:         Wether previous entries for this model should be deleted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates the directory for scores if it doesn't exists.\n",
    "    if not os.path.exists(SCORE_DIR):\n",
    "        os.makedirs(SCORE_DIR)\n",
    "        \n",
    "    # Prepare the row to save in the data frame\n",
    "    new_row = [summarizer_name, time, average_score[\"f\"], average_score[\"p\"], average_score[\"r\"],\n",
    "               str(text_cut), sampling_size]\n",
    "    \n",
    "    # Loads the csv file if it exists\n",
    "    if os.path.exists(SCORE_FILE):\n",
    "        df = pd.read_csv(SCORE_FILE, index_col=0)\n",
    "        df = df.append(dict(zip(COLS_NAME, new_row)), ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame(np.array([new_row]), columns = COLS_NAME)\n",
    "    df.to_csv(SCORE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####brutal-5####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 760.06s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.17234754325097182, 'p': 0.16253698583847748, 'r': 0.19516229287431888}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 175.75s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16441473901986703, 'p': 0.16267730437434041, 'r': 0.17674089788321118}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 160.55s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.17332370000275393, 'p': 0.1660625065840514, 'r': 0.19308502312115727}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/truncated_svd.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/truncated_svd.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/truncated_svd.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/truncated_svd.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time : 178.49s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16991230556385228, 'p': 0.1648666713340218, 'r': 0.18654154003954124}\n",
      "\n",
      "####brutal-10####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 39.74s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16562356642849696, 'p': 0.15379604051194687, 'r': 0.19174242044125045}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 49.97s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1642379684504946, 'p': 0.15497989490485933, 'r': 0.18691224825164765}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 37.01s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16525234603159603, 'p': 0.15487657020851037, 'r': 0.18920233231495928}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 46.11s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1658571207730447, 'p': 0.15570809590089285, 'r': 0.1896090528830337}\n",
      "\n",
      "####brutal-15####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 21.88s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16638497482647915, 'p': 0.15193949522892466, 'r': 0.19621250292997455}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 27.93s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16647200266701903, 'p': 0.15329267508195626, 'r': 0.19436173541936994}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 22.06s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16667870841846236, 'p': 0.1530418680073499, 'r': 0.19535668539329473}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 27.53s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16794586284879526, 'p': 0.15435500035649571, 'r': 0.1965957394588429}\n",
      "\n",
      "####overlap-5-2####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 270.41s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.17556686053320003, 'p': 0.17306902491982823, 'r': 0.19019252782528617}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 362.26s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16312414857530866, 'p': 0.1638745557722091, 'r': 0.17336371895264843}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 271.84s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.17411407851863706, 'p': 0.17199159435091663, 'r': 0.18837342128678078}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 388.81s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16583391600737132, 'p': 0.16646952337459425, 'r': 0.1762237872831281}\n",
      "\n",
      "####overlap-10-4####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 181.32s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16749985975864368, 'p': 0.1577542419956175, 'r': 0.19138618033365037}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 192.36s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1636069933188277, 'p': 0.15475524773907326, 'r': 0.1866034498071363}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 159.92s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16657192172534346, 'p': 0.15734539443158754, 'r': 0.18993273624890442}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 191.37s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16568272491641606, 'p': 0.15682205048197326, 'r': 0.18870626288088624}\n",
      "\n",
      "####overlap-15-6####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 105.57s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1614822479063247, 'p': 0.15254027909760115, 'r': 0.18359708031289906}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 145.07s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1602505893227536, 'p': 0.1514455619693018, 'r': 0.1822634015839888}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 120.68s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1627610574198683, 'p': 0.15378584981698468, 'r': 0.1851452324745952}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 158.96s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1629308377363635, 'p': 0.15397018694958325, 'r': 0.1852155913966325}\n",
      "\n",
      "####nltk####\n",
      "Loading done\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 25.59s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.15935372608768977, 'p': 0.14276092930752107, 'r': 0.19138171494096726}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,none, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 29.62s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1607701476222344, 'p': 0.1445926916659664, 'r': 0.19214057251043948}\n",
      "\n",
      "TextRank_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 21.84s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.1594853299644623, 'p': 0.14323615088750435, 'r': 0.19105268349734383}\n",
      "\n",
      "LSA_TFIDF_Summarizer_Assym_process(1,0.0,unweighted,before, 1.5 )\n",
      "Preprocess done\n",
      "\n",
      "Time : 28.96s\n",
      "Summary done\n",
      "Rouge preprocess done\n",
      "Rouge done : {'f': 0.16017019953793252, 'p': 0.14367355306769897, 'r': 0.19220434455355379}\n"
     ]
    }
   ],
   "source": [
    "len_summary = 20\n",
    "\n",
    "def run_summaries(option, len_summary, rouge_score, corpus = \"dmoz-html\", language = \"french\", sampling = 0.1):\n",
    "    \"\"\"\n",
    "    :param option:       List of summarizer process to be tested.\n",
    "    :param len_summary:  Number of words forming the summary.\n",
    "    :param rouge_score:  Empty dictionary in wich ROUGE L median scores will be stored.\n",
    "    :param corpus:       String to indentify the corpus to use.\n",
    "    :param language:     String to indicate the language.\n",
    "                         If set, allows to use the stop words of the given language.\n",
    "                         Default value is \"french\".\n",
    "    :param sampling:     Threshold value for document selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of hypotetic / predicted summaries generated with a summarizer\n",
    "    pred_sum = []\n",
    "    k = 0\n",
    "    \n",
    "    # Loads stop words for a given language if set.\n",
    "    stops = set() if language is None else set(stopwords.words(language))\n",
    "    \n",
    "    # Choose the 'cut' for the corpuses.\n",
    "    # Choix du d√©coupage des corpus\n",
    "    \n",
    "    #for gen in [('brutal', 10)]:\n",
    "    for gen in [('brutal',i*5) for i in range(1,4)] + [('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] :\n",
    "        print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "        \n",
    "        # Corpus generation.\n",
    "        if corpus == \"dmoz\" :\n",
    "            docs, gold_tokenized_summaries, overall = generate_corpus(*gen, sampling = sampling)\n",
    "        elif corpus == \"dmoz-html\":\n",
    "            docs, golf_tokenized_summaries, overall, segments = generate_corpus_dmoz_html(*gen, sampling = sampling)\n",
    "        else :\n",
    "            docs, gold_tokenized_summaries, overall  = generate_corpus_duc(*gen, sampling = sampling)\n",
    "        \n",
    "        \n",
    "        for summarizer in option :\n",
    "            \n",
    "            # Creation of hypotetic summaries with the summarizer being tested.\n",
    "            print(\"\\n\"+summarizer.__name__)\n",
    "            s = time.time()\n",
    "            pred_sum.append(doc_summarizer(docs, summarizer, len_summary, 0 if len(gen) < 3 else gen[2], segments))\n",
    "            e = time.time()\n",
    "            print(\"\\nTime :\", \"{:.2f}s\".format(e-s))\n",
    "            print(\"Summary done\")\n",
    "            \n",
    "            # Cr√©ation des phrases\n",
    "            all_hypothesis = []\n",
    "            all_references = []\n",
    "            \n",
    "            # For each document (doc_id) in our documents and summaries sets\n",
    "            for ref in set(overall.keys()).intersection(docs.keys()) :\n",
    "                if len(gen)>2 :\n",
    "\n",
    "                    pred_sum_words = set()\n",
    "                    sen = \"\"\n",
    "                    for sen_id in pred_sum[k][ref]:\n",
    "                        segment = docs[ref][1][sen_id].split()\n",
    "                        for i in range(sen_id * gen[2], sen_id * gen[2] + gen[1]) :\n",
    "                            if i in pred_sum_words :\n",
    "                                pass\n",
    "                            else :\n",
    "                                pred_sum_words.add(i)\n",
    "                                sen += \" \" + segment[i - sen_id * gen[2]]\n",
    "                                if len(pred_sum_words) == len_summary :\n",
    "                                    break\n",
    "                        else :\n",
    "                            continue\n",
    "                        break\n",
    "\n",
    "                    hypothesis = sen\n",
    "\n",
    "                else :\n",
    "                    hypothesis = join_and_cut_at([docs[ref][1][sen_id] for sen_id in pred_sum[k][ref]],\n",
    "                                                          len_summary, stops)\n",
    "                for anot in gold_tokenized_summaries[ref] :\n",
    "                    all_hypothesis.append(hypothesis)\n",
    "                    all_references.append(join_and_cut_at(gold_tokenized_summaries[ref][anot][1], len_summary, stops))\n",
    "            print(\"Rouge preprocess done\")\n",
    "\n",
    "            #Calcul des scores rouges.\n",
    "            under = [x[\"rouge-1\"] for x in evaluator.get_scores(all_hypothesis, all_references)]\n",
    "            med = { k : sum(t[k] for t in under)/len(under) for k in under[0] }\n",
    "            rouge_score[\"-\".join([str(x) for x in gen])][summarizer.__name__] = {\"rouge-1\" : med}\n",
    "            print(\"Rouge done : \"+ str(med))\n",
    "            save_summarizer_score(summarizer.__name__, \"{:.2f}s\".format(e-s), med, \"-\".join([str(x) for x in gen]), sampling)\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "    # Return predicted summaries\n",
    "    return pred_sum\n",
    "\n",
    "resumes = run_summaries(option, len_summary, rouge_score, language=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumy\n",
    "\n",
    "Testing of the [sumy library](#https://pypi.org/project/sumy/).\n",
    "\n",
    "Why is it not done with the other summarizer processes ? God only knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sumy_summaries = []\n",
    "\n",
    "# Loads stop words for a given language if set.\n",
    "stops = set()\n",
    "\n",
    "# Choose the 'cut' for the corpuses.\n",
    "# Choix du d√©coupage des corpus\n",
    "for gen in [('brutal',i*5) for i in range(1,4)]+[('overlap', i*5, i*2) for i in range(1,4)]+[('nltk',)] :\n",
    "    print(\"\\n####\"+\"-\".join([str(x) for x in gen])+\"####\")\n",
    "\n",
    "docs, gold_tokenized_summaries, overall = generate_corpus(*gen, sampling = 1)\n",
    "for summarizer in summy_option :\n",
    "    print(\"\\n\"+summarizer.__name__)\n",
    "    d = {}\n",
    "    i = 0\n",
    "    l = len(docs)\n",
    "    for ref in docs.keys() :\n",
    "        i+=1\n",
    "        d[ref] = summarizer.summarize(ref, docs, True)\n",
    "        print(\"\\r{:.2%}\".format(i/l),end = \" \")\n",
    "    sumy_summaries.append(d)\n",
    "    \n",
    "    print(\"\\nSummary done\")\n",
    "    all_hypothesis = []\n",
    "    all_references = []\n",
    "    for ref in overall.keys() :\n",
    "        hypothesis = [ \"\\n\".join(sumy_summaries[-1][ref])] \n",
    "        references = [[ \"\\n\".join(gold_tokenized_summaries[ref][ano][0]) for ano in gold_tokenized_summaries[ref] ]]\n",
    "        all_hypothesis += hypothesis\n",
    "        all_references += references\n",
    "    rouge_score[option.__name__] = evaluator.get_scores(all_hypothesis, all_references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "    \n",
    "#Used to turn the dictionary into a usable dataframe.\n",
    "rowsR = []\n",
    "for g in rouge_score :\n",
    "    for m in rouge_score[g] :\n",
    "        for s in rouge_score[g][m] :\n",
    "            rowsR.append({\"M√©thode\" : m, \"Score\" : g, \"Recall\" : rouge_score[g][m][s]['r']})\n",
    "replacement = ([\"brutal-5\", \"overlap-5-2\"],[\"brutal-05\", \"overlap-05-2\"])\n",
    "dataR = pd.DataFrame(rowsR, columns = [\"M√©thode\",\"Score\", \"Recall\"]).replace(*replacement)\n",
    "\n",
    "#For showing datas in a graph\n",
    "sns.lineplot(data=dataR, x=\"Score\", y=\"Recall\",hue=\"M√©thode\" )\n",
    "plt.legend(bbox_to_anchor=(0.01, 0.3), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfR = pd.DataFrame(index = sorted(set(dataR[\"M√©thode\"])),\n",
    "                   columns = sorted(set(dataR[\"Score\"]))).rename_axis(\"Rappel\", axis=\"columns\")\n",
    "for r in dataR.iterrows()  :\n",
    "    dfR.at[r[1][\"M√©thode\"], r[1][\"Score\"]] = r[1][\"Recall\"]\n",
    "from IPython.display import display\n",
    "display(dfR.sort_values(['brutal-05'],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
