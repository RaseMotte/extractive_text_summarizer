{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "import seaborn\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85, max_iter = 5000):\n",
    "    \"\"\"\n",
    "    PageRank algorithm.\n",
    "    \n",
    "    Given a similarity matrix, returns list of score for each sentence.\n",
    "\n",
    "    :param A:         matrix (n, n)\n",
    "                      The adjacency matrix of the graph on which to compute PageRank score\n",
    "                      \n",
    "    :param eps:       float, optional\n",
    "                      Tolerance. The algorithm stops as soon as the update magnitude of all\n",
    "                      values is below this threshold.\n",
    "              \n",
    "    :param d:         float, optional\n",
    "                      1 - probability of teleporting to a random node\n",
    "            \n",
    "    :param max_iter:  int, optional\n",
    "                      Maximum number of iterations\n",
    "                      \n",
    "    :return:    A matrix (n, n).\n",
    "                The ranking of sentences in the document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # P is the vector of probability to \"teleport\" on each node. By default filled of 1/n.\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    \n",
    "    while max_iter > 0 :\n",
    "        max_iter-=1\n",
    "        # Markov chain transition\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.dot(P)\n",
    "        # Normalization\n",
    "        new_P = new_P / np.linalg.norm(new_P)\n",
    "        # Compute mean absolute error (MAE)\n",
    "        delta = abs(new_P - P).sum() / len(new_P)\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P\n",
    "    \n",
    "    print(\"Convergence error : \" + str(delta))\n",
    "    return new_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_summarizer(matrix, corpus, weights=None, nb_words = 100, diag = \"none\", bias = None):\n",
    "    \"\"\"\n",
    "    Prepare the sentence-term matrix before applying PageTank algorithm to it.\n",
    "    \n",
    "    :param matrix:    matrix (n, m)\n",
    "                      A sentence-term matrix weighting the importance of a term for a sentence.\n",
    "                      \n",
    "    :param corpus:    list of string.\n",
    "                      A single document.\n",
    "    \n",
    "    :param weights:   ???\n",
    "    :param nb_words:  The number of words for the summary.\n",
    "    :param diag:      ???\n",
    "    :param biais:     ???\n",
    "    \"\"\"\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    if diag == \"before\":\n",
    "         np.fill_diagonal(matrix,0)   \n",
    "    \n",
    "    # Similarity matrix <=> Adjacancy matrix ???\n",
    "    sim_matrix = normalize(matrix , norm = 'l1', axis = 0)\n",
    "    \n",
    "    if (not weights is None) and (len(weights) == matrix.shape[0]):\n",
    "        sim_matrix = np.matmul(sim_matrix,np.diag(weights))\n",
    "    \n",
    "    if diag == \"after\" :\n",
    "        np.fill_diagonal(matrix,0)\n",
    "\n",
    "    if bias is not None:\n",
    "        sim_matrix = sim_matrix + bias\n",
    "    sim_matrix = sim_matrix / matrix.shape[0]\n",
    "    results = pagerank(sim_matrix)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA : Latent Semantic Analysis\n",
    "\n",
    "Source: [Text Summarization Techniques: A Brief Survey](https://arxiv.org/pdf/1707.02268.pdf)\n",
    "\n",
    "## 1. Builds term-sentence matrix\n",
    "\n",
    "$X \\in \\mathbb{M}^{n \\times m}$: term sentence matrix\n",
    "* rows correspond to a word from the input (n words)\n",
    "* columns correspond to a sentence (m sentences)\n",
    "\n",
    "$x_{ij}$: weight of the word j in sentence i\n",
    "* weights of the words computed with TFIDF.\n",
    "* if a sentence does not have a word the weight of that word in the sentence is zero.\n",
    "\n",
    "## 2. Sentence correlation matrix over the vocabulary\n",
    "\n",
    "$XX^T$\n",
    "\n",
    "## 3. SVD: singular value decomposition\n",
    "\n",
    "Transforms the matrix X into three matrices: $A = U\\Sigma V^T$.\n",
    "\n",
    "$XX^T = (U\\Sigma V^T)(U\\Sigma V^T)^T = (U\\Sigma V^T) (V \\Sigma^T U^T) = U\\Sigma \\Sigma^T U^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_summarizer(matrix, corpus, weights=None, nbcompfun = None, nb_words = 100, diag = \"none\", bias = None):\n",
    "    \"\"\"\n",
    "    :param matrix:    matrix (n, m)\n",
    "                      sentence-term matrix weighting the importance of a term for a sentence.\n",
    "    \n",
    "    :param corpus:    list of string.\n",
    "                      A single document.\n",
    "    :param weights:   ???\n",
    "    :param nbcompfun: ???\n",
    "    :param nb_words:  The number of words for the summary.\n",
    "    :param diag:      ???\n",
    "    :param biais:     ???\n",
    "    \n",
    "    \"\"\"\n",
    "    #MÃ©thode de calcul LSA\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    if nbcompfun == None:\n",
    "        nbcompfun = lambda x : log(x)\n",
    "    \n",
    "    k = max(1, int(nbcompfun(len(corpus))))\n",
    "    \n",
    "    if k >= len(corpus):\n",
    "        return np.sum(matrix,axis=1)\n",
    "    \n",
    "    if diag == \"before\" :\n",
    "        np.fill_diagonal(matrix,0) \n",
    "\n",
    "    sim_matrix = normalize(matrix , norm = 'l1', axis = 0)\n",
    "    \n",
    "    if (not weights is None) and (len(weights) == matrix.shape[0]):\n",
    "        sim_matrix = np.matmul(sim_matrix,np.diag(weights))\n",
    "    \n",
    "    if diag == \"after\" :\n",
    "        np.fill_diagonal(matrix,0)\n",
    "    if bias is not None:\n",
    "        sim_matrix = sim_matrix + bias\n",
    "    sim_matrix = sim_matrix / matrix.shape[0]\n",
    "\n",
    "    #tsvd = sklearn.decomposition.TruncatedSVD(k, random_state=1337)\n",
    "    tsvd = sklearn.decomposition.TruncatedSVD(k, random_state=1337, algorithm = \"arpack\")\n",
    "    \n",
    "    # Dimension reduction of sim_matrix using truncated SVD.\n",
    "    # results = topic-sentence matrix\n",
    "    results = tsvd.fit_transform(sim_matrix)\n",
    "    \n",
    "    # scores = sigular_values @ result.T\n",
    "    # scores matrix describe how much a sentence represent a word, thus\n",
    "    # the weight / importance of a word in a sentence.\n",
    "    scores = np.abs(np.matmul(results, np.diag(np.sqrt(tsvd.singular_values_))))\n",
    "    \n",
    "    # Give a score per sentence as the sum of the weights of the words it contains\n",
    "    # score_sent = sum(row)\n",
    "    # REMARK : Text Summarization Techniques: A Brief survey (paper) suggest sqrt(sum(scores_ij^2))\n",
    "    maxscores = np.sum(scores,axis=1)\n",
    "\n",
    "    return maxscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_summarizer(method, matrix, corpus, weighted, lsanbcompfun = lambda x : log(x), diag = \"none\", bias = None):\n",
    "    \"\"\"\n",
    "    :param method:        String defining if summarization is done with LSA or TextRank.\n",
    "    :param matrix:        Adjancy matrix. Distance between sentences.\n",
    "    :param corpus:        Array of string. One textual document.\n",
    "    :param weighted:      Boolean.\n",
    "    :param lsandcompfun:\n",
    "    :param diag:\n",
    "    :param bias:    Dictionay mapping element of interests (ex html tag) to value, weight.\n",
    "    \"\"\"\n",
    "    \n",
    "    segments, bias_dict = bias\n",
    "    if bias_dict is not None:\n",
    "        doc_bias = get_bias(corpus, segments, bias_dict)\n",
    "    else:\n",
    "        doc_bias = None\n",
    "    #Is used to determine which method has to be used\n",
    "    if not lsanbcompfun : \n",
    "        lsanbcompfun = lambda x : 1\n",
    "    if method == \"tr\":\n",
    "            if weighted:\n",
    "                return tr_summarizer(matrix, corpus, get_weights(corpus), diag = diag, bias = doc_bias)\n",
    "            else:\n",
    "                return tr_summarizer(matrix, corpus, diag = diag, bias = doc_bias)\n",
    "    else:\n",
    "        if method == \"lsa\":\n",
    "            if weighted:\n",
    "                return lsa_summarizer(matrix, corpus, get_weights(corpus),\n",
    "                                      nbcompfun = lsanbcompfun, diag = diag , bias = doc_bias)\n",
    "            else:\n",
    "                return lsa_summarizer(matrix, corpus, nbcompfun = lsanbcompfun, diag = diag, bias = doc_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines bias matrix for the document, given a bias dictionary and the segments of interest of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(doc, segments, bias_dict):\n",
    "    \"\"\"\n",
    "    Defines the bias matrix for the document.\n",
    "    bias param is not the matrix. It's named bias only to respect the old format defined by prev devs.\n",
    "\n",
    "    Given a dictionary mapping sentences token to tag, builds a vector with\n",
    "    ith sentence weight at pos i.\n",
    "    \n",
    "    :param corpus:    one preprocessed document, as given from the pool shit bellow.\n",
    "    :param segments:  Given an element (for instance, and html tag), maps the sentences of the\n",
    "                      document which were concerned by the document.\n",
    "    :param bias:      Given an element (for instance, an html tag), return a weight value.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    w = np.zeros(len(doc))\n",
    "    for tag, sents in segments.items():\n",
    "        # get sentences position in document\n",
    "        for sent in sents:\n",
    "            if len(sent) < 1 or sent not in doc:\n",
    "                continue\n",
    "            sent_id = doc.index(sent)\n",
    "            w[sent_id] = bias_dict[tag]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0\n",
    "\n",
    "We define $W \\in \\mathbf{R}^n$ the intermediate weight vector:\n",
    "\n",
    "$w_{i} = \\large \\frac{len(s_i)}{\\sqrt{n + 1 - i}} $\n",
    "\n",
    "* $d \\in \\mathbb{R}^n$: document\n",
    "* $s_i$: ith sentence of document d, $i \\in [[0, n]]$\n",
    "* $w_{i}$: weight for ith sentence\n",
    "\n",
    "\n",
    "Step 1\n",
    "We define $\\Omega \\in \\mathbf{R}^n$ the final weight vector:\n",
    "\n",
    "$\\Omega_i = n \\times \\large \\frac{w_{i}}{\\sum_{i = 1}^{n} w_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(corpus):\n",
    "    \"\"\"\n",
    "    Build a weight matrix for a document.\n",
    "    Attributes to each sentence a weight.\n",
    "    Weights are defined <???>\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    nbsent = len(corpus)\n",
    "    sentindex = nbsent + 1\n",
    "    for sent in corpus :\n",
    "        splitsent = sent.split()\n",
    "        weights.append(len(splitsent) / ((sentindex**0.5)))\n",
    "        sentindex -= 1.0        \n",
    "    weights = np.array(weights)\n",
    "    weights = nbsent * weights / np.sum(weights)\n",
    "    return weights  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
